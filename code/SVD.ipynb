{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "from gensim_download import pickle_rw\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy.linalg\n",
    "\n",
    "from numpy.linalg import det\n",
    "\n",
    "from numpy.linalg import inv\n",
    "\n",
    "import sympy as sympy\n",
    "\n",
    "from sympy import Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dict(vocab, vectors):\n",
    "    \"\"\"Make dictionary of vocab and vectors\"\"\"\n",
    "    return {vocab[i]: vectors[i] for i in range(len(vocab))}\n",
    "\n",
    "\n",
    "def vocab_train_test(embedding, lg1, lg2, lg1_vocab):\n",
    "    \"\"\"Create training and test vocabularies\"\"\"\n",
    "    if embedding == 'zeroshot':\n",
    "        with open('../data/zeroshot/transmat/data/' +\n",
    "                  'OPUS_en_it_europarl_train_5K.txt') as f:\n",
    "            vocab_train = [(_.split(' ')[0], _.split(' ')[1])\n",
    "                           for _ in f.read().split('\\n')[:-1]]\n",
    "        with open('../data/zeroshot/transmat/data/' +\n",
    "                  'OPUS_en_it_europarl_test.txt') as f:\n",
    "            vocab_test = [(_.split(' ')[0], _.split(' ')[1])\n",
    "                          for _ in f.read().split('\\n')[:-1]]\n",
    "\n",
    "    elif embedding in ['fasttext_random', 'fasttext_top']:\n",
    "        embedding, split = embedding.split('_')\n",
    "        lg1_lg2, lg2_lg1 = pickle_rw((lg1 + '_' + lg2, 0),\n",
    "                                     (lg2 + '_' + lg1, 0), write=False)\n",
    "        # T = Translation, R = Reverse (translated and then translated back)\n",
    "        # Create vocab from 2D translations\n",
    "        vocab_2D = []\n",
    "        for lg1_word in lg1_vocab:\n",
    "            # Translate lg1_word\n",
    "            if lg1_word in lg1_lg2:\n",
    "                lg1_word_T = lg1_lg2[lg1_word]\n",
    "\n",
    "                # Check if translated word (or lowercase) is in lg2_lg1\n",
    "                if lg1_word_T in lg2_lg1.keys():\n",
    "                    lg1_word_R = lg2_lg1[lg1_word_T]\n",
    "                elif lg1_word_T.lower() in lg2_lg1.keys():\n",
    "                    lg1_word_T = lg1_word_T.lower()\n",
    "                    lg1_word_R = lg2_lg1[lg1_word_T]\n",
    "                else:\n",
    "                    lg1_word_R = None\n",
    "\n",
    "                # Check if lg1_word and lg1_word_R are equal (lowercase)\n",
    "                if lg1_word_R:\n",
    "                    if lg1_word.lower() == lg1_word_R.lower():\n",
    "                        vocab_2D.append((lg1_word, lg1_word_T))\n",
    "        print('length of '+ lg1+'-'+ lg2+ ' vocab: '+str(len(vocab_2D)))\n",
    "\n",
    "        #Create Train/Test vocab\n",
    "\n",
    "        if split == 'random':\n",
    "            sample = np.random.choice(len(vocab_2D), 6500, replace=False)\n",
    "            vocab_train = np.asarray(vocab_2D)[sample[:5000]].tolist()\n",
    "            vocab_test = np.asarray(vocab_2D)[sample[5000:]].tolist()\n",
    "        elif split == 'top':\n",
    "            sample = np.random.choice(range(6500), 6500, replace=False)\n",
    "            vocab_train = np.asarray(vocab_2D)[:5000, :].tolist()\n",
    "            vocab_test = np.asarray(vocab_2D)[:1500, :].tolist()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # if split == 'random':\n",
    "        #     sample = np.random.choice(len(vocab_2D), 900, replace=False)\n",
    "        #     vocab_train = np.asarray(vocab_2D)[sample[:700]].tolist()\n",
    "        #     vocab_test = np.asarray(vocab_2D)[sample[700:]].tolist()\n",
    "        # elif split == 'top':\n",
    "        #     sample = np.random.choice(range(900), 900, replace=False)\n",
    "        #     vocab_train = np.asarray(vocab_2D)[:700, :].tolist()\n",
    "        #     vocab_test = np.asarray(vocab_2D)[:200, :].tolist()\n",
    "        # else:\n",
    "        #     pass\n",
    "\n",
    "    return vocab_train, vocab_test\n",
    "\n",
    "\n",
    "def vectors_train_test(vocab_train, vocab_test):\n",
    "    \"\"\"Create training and test vectors\"\"\"\n",
    "    X_train, y_train = zip(*[(lg1_dict[lg1_word], lg2_dict[lg2_word])\n",
    "                             for lg1_word, lg2_word in vocab_train])\n",
    "    X_test, y_test = zip(*[(lg1_dict[lg1_word], lg2_dict[lg2_word])\n",
    "                           for lg1_word, lg2_word in vocab_test])\n",
    "    return map(np.asarray, (X_train, X_test, y_train, y_test))\n",
    "\n",
    "\n",
    "def translation_matrix(X_train, y_train):\n",
    "    \"\"\"Fit translation matrix T\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, use_bias=False, input_shape=(X_train.shape[1],),kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    history = model.fit(X_train, y_train, batch_size=128, epochs=20,\n",
    "                        verbose=False)\n",
    "    T = model.get_weights()[0]\n",
    "\n",
    "    T = np.matrix(T)\n",
    "\n",
    "    M = np.multiply(np.matrix(T),100)\n",
    "\n",
    "    T_norm, T_normed = normalize(M)\n",
    "\n",
    "    D = np.linalg.det(M)\n",
    "    \n",
    "    I = inv(T)\n",
    "    \n",
    "    #Fr_norm = np.linalg.det(np.subtract(np.matmul(M,M.getH()),np.matmul(M.getH(),M)))\n",
    "    \n",
    "    Fr_norm = np.linalg.det(np.matrix(np.subtract(np.matmul(T,T.getH()),np.matmul(T.getH(),T))))\n",
    "    \n",
    "    #print(T_normed)\n",
    "    #print(T_normed.getH())\n",
    "\n",
    "    #print(np.around(np.matmul(T_normed,T_normed.getH())))\n",
    "\n",
    "    #print(np.around(np.matmul(T_normed.getH(),T_normed)))\n",
    "\n",
    "    print (\"Determinant:\"+str(D))\n",
    "    \n",
    "    #print (\"Fr_norm:\"+str(Fr_norm))\n",
    "\n",
    "    if np.array_equal(np.around(np.matmul(T_normed,T_normed.getH())), np.around(np.matmul(T_normed.getH(),T_normed))) == True:\n",
    "        tf = \"True\"\n",
    "    else:\n",
    "        tf = \"False\"\n",
    "\n",
    "    return model, history, T, D, tf, I, M\n",
    "\n",
    "def translation_accuracy(X_test, y_test):\n",
    "    \"\"\"Get predicted matrix 'yhat' using 'T' and find translation accuracy\"\"\"\n",
    "    # yhat\n",
    "    yhat = X.dot(T)\n",
    "    count = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if yhat[i,:].all() == y_test[i,:].all():\n",
    "            count = count + 1\n",
    "    accuracy = count/len(y_test)*100\n",
    "    return accuracy\n",
    "\n",
    "def svd(T):\n",
    "    \"\"\"Perform SVD on the translation matrix 'T' \"\"\"\n",
    "    U, s, Vh = numpy.linalg.svd(T, full_matrices=False )\n",
    "    return U, s, Vh\n",
    "\n",
    "def T_svd_EDA(s):\n",
    "    \"\"\"Perform SVD on the translation matrix 'T' \"\"\"\n",
    "    plt.hist(s, bins='auto', range = (0,1),normed = 1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize(matrix):\n",
    "    \"\"\"Normalize the rows of a matrix\"\"\"\n",
    "    matrix_norm = np.linalg.norm(matrix, axis=1)\n",
    "    matrix_normed = matrix / np.repeat(matrix_norm, matrix.shape[1]). \\\n",
    "        reshape(matrix.shape)\n",
    "    return matrix_norm, matrix_normed\n",
    "\n",
    "\n",
    "def translation_results(X, y, vocab, M, lg2_vectors, lg2_vocab):\n",
    "    \"\"\"X, y, vocab - The training or test data that you want results for\n",
    "    T - The translation matrix\n",
    "    lg2_vectors, lg2_vocab - Foreign language used to find the nearest neighbor\n",
    "    \"\"\"\n",
    "\n",
    "    # Data Prep on Inputs\n",
    "    X_word, y_word = zip(*vocab)\n",
    "    X_norm, X_normed = normalize(X)\n",
    "    y_norm, y_normed = normalize(y)\n",
    "    lg2_vectors_norm, lg2_vectors_normed = normalize(lg2_vectors)\n",
    "\n",
    "    # yhat\n",
    "    yhat = X.dot(M)\n",
    "    yhat_norm, yhat_normed = normalize(yhat)\n",
    "\n",
    "    #X_norm = normalize(X)\n",
    "\n",
    "\n",
    "    # Nearest Neighbors\n",
    "#     neg_cosine = -yhat_normed.dot(lg2_vectors_normed.T)\n",
    "#     ranked_neighbor_indices = np.argsort(neg_cosine, axis=1)\n",
    "\n",
    "#     # Nearest Neighbor\n",
    "#     nearest_neighbor_indices = ranked_neighbor_indices[:, 0]\n",
    "#     yhat_neighbor = lg2_vectors[nearest_neighbor_indices, :]\n",
    "#     yhat_neighbor_norm, yhat_neighbor_normed = normalize(yhat_neighbor)\n",
    "#     yhat_neighbor_word = np.asarray(lg2_vocab)[nearest_neighbor_indices]\n",
    "\n",
    "#     # Results DF\n",
    "#     cols = ['X_norm', 'y_norm', 'yhat_norm', 'yhat_neighbor_norm',\n",
    "#             'X_word', 'y_word', 'yhat_neighbor_word']\n",
    "#     results_df = pd.DataFrame({'X_norm': X_norm,\n",
    "#                                'y_norm': y_norm,\n",
    "#                                'yhat_norm': yhat_norm,\n",
    "#                                'yhat_neighbor_norm': yhat_neighbor_norm,\n",
    "#                                'X_word': X_word,\n",
    "#                                'y_word': y_word,\n",
    "#                                'yhat_neighbor_word': yhat_neighbor_word,})\n",
    "#     results_df = results_df[cols]\n",
    "#     results_df['neighbor_correct'] = results_df.y_word == \\\n",
    "#         results_df.yhat_neighbor_word\n",
    "\n",
    "    return yhat_norm\n",
    "\n",
    "\n",
    "def T_norm_EDA(results_df):\n",
    "    \"\"\"Plot result norms side-by-side\"\"\"\n",
    "    test_size = results_df.shape[0]\n",
    "    test_accuracy = round(results_df.neighbor_correct.mean(), 2)\n",
    "\n",
    "    print('Test Accuracy: '+str(test_accuracy)+'\\n')\n",
    "\n",
    "    plot_data = ['X_norm', 'y_norm', 'yhat_norm', 'yhat_neighbor_norm']\n",
    "    # f, ax = plt.subplots(len(plot_data), sharex=True, sharey=True,\n",
    "    #                      figsize=(10, 10))\n",
    "    # for i, d in enumerate(plot_data):\n",
    "    #     ax[i].hist(results_df[d], bins=100)\n",
    "    #     ax[i].axis('off')\n",
    "    #     title = '{}: mean={}, std={}'.format(d, round(results_df[d].mean(), 2), round(results_df[d].std(), 2))\n",
    "    #     ax[i].set_title(title)\n",
    "    # f.subplots_adjust(hspace=0.7)\n",
    "    # plt.savefig('../images/' + lg1 + '_' + lg2 + '_' + embedding +\n",
    "    #             '_T_norm.png')\n",
    "    # plt.close('all')\n",
    "    return\n",
    "\n",
    "\n",
    "def T_pca_EDA(T):\n",
    "    \"\"\"PCA on matrix T\"\"\"\n",
    "    T_ss = StandardScaler().fit_transform(T)\n",
    "    pca = PCA().fit(T_ss)\n",
    "    n = pca.n_components_\n",
    "\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.xlim((0, n))\n",
    "    # plt.ylim((0, 1))\n",
    "    # plt.plot(range(n + 1), [0] + np.cumsum(pca.explained_variance_ratio_).\n",
    "    #          tolist())\n",
    "    # plt.plot(range(n + 1), np.asarray(range(n + 1)) / n)\n",
    "    # plt.xlabel('Number of Eigenvectors')\n",
    "    # plt.ylabel('Explained Variance')\n",
    "    # plt.savefig('../images/' + lg1 + '_' + lg2 + '_' + embedding +\n",
    "    #             '_T_isotropy.png')\n",
    "    # plt.close('all')\n",
    "\n",
    "    isotropy = (1 - sum(np.cumsum(pca.explained_variance_ratio_) * 1 / n)) / .5\n",
    "    return isotropy\n",
    "\n",
    "\n",
    "def T_report_results(embedding, lg1, lg2, lg1_vectors, lg2_vectors,\n",
    "                     X_train, X_test, D, results_df, isotropy):\n",
    "    md = '## ' + lg1.title() + ' to ' + lg2.title() + ' ' + \\\n",
    "        embedding.title() + '  \\n'\n",
    "    md += '- ' + lg1.title() + ' Vocabulary Size = ' + \\\n",
    "        '{:,.0f}'.format(lg1_vectors.shape[0]) + '  \\n'\n",
    "    md += '- ' + lg1.title() + ' Embedding Length = ' + \\\n",
    "        '{:,.0f}'.format(lg1_vectors.shape[1]) + '  \\n'\n",
    "    md += '- ' + lg2.title() + ' Vocabulary Size = ' + \\\n",
    "        '{:,.0f}'.format(lg2_vectors.shape[0]) + '  \\n'\n",
    "    md += '- ' + lg2.title() + ' Embedding Length = ' + \\\n",
    "        '{:,.0f}'.format(lg2_vectors.shape[1]) + '  \\n'\n",
    "    md += '- Train Size = ' + '{:,.0f}'.format(X_train.shape[0]) + '  \\n'\n",
    "    md += '- Test Size = ' + '{:,.0f}'.format(X_test.shape[0]) + '  \\n'\n",
    "    md += '- Determinant = ' + '{:,.0f}'.format(D) + '  \\n'\n",
    "\n",
    "    md += '- <b>Test Accuracy = ' + \\\n",
    "        '{:,.1%}'.format(results_df.neighbor_correct.mean()) + '</b>  \\n\\n'\n",
    "\n",
    "\n",
    "\n",
    "    md += '#### Test L2 Norms  \\n'\n",
    "    md += '- X_norm: L2 norms for ' + lg1.title() + ' test vectors  \\n'\n",
    "    md += '- y_norm: L2 norms for ' + lg2.title() + ' test vectors  \\n'\n",
    "    md += '- yhat_norm: L2 norms for X.dot(T) test vectors ' + \\\n",
    "        '(T = translation matrix)  \\n'\n",
    "    md += '- yhat_neighbor norm: L2 norms for nearest neighbor' + \\\n",
    "        'to X.dot(T) in y test vectors  \\n'\n",
    "    md += '![](../images/' + lg1 + '_' + lg2 + '_' + embedding + \\\n",
    "        '_T_norm.png)  \\n\\n'\n",
    "\n",
    "    md += '#### Translation Matrix Isotropy  \\n'\n",
    "    md += '- Isotropy = ' + '{:,.1%}'.format(isotropy) + '  \\n'\n",
    "    md += '![](../images/' + lg1 + '_' + lg2 + '_' + embedding + \\\n",
    "        '_T_isotropy.png)  \\n\\n'\n",
    "\n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function ( with SVD stats )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Manually set list of translations (embedding, lg1, lg2)\n",
    "    translations = [#('fasttext_random', 'en', 'ru'),\n",
    "                    ('fasttext_top', 'en', 'ru'),\n",
    "                    #('fasttext_random', 'en', 'de'),\n",
    "                    ('fasttext_top', 'en', 'de'),\n",
    "                    #('fasttext_random', 'en', 'es'),\n",
    "                    ('fasttext_top', 'en', 'es'),\n",
    "                    #('fasttext_random', 'en', 'zh-CN'),\n",
    "                    ('fasttext_top', 'en', 'zh-CN'),\n",
    "                    #('fasttext_random', 'de', 'en'),\n",
    "                    ('fasttext_top', 'de', 'en'),\n",
    "                    #('fasttext_random', 'de', 'es'),\n",
    "                    ('fasttext_top', 'de', 'es'),\n",
    "                    #('fasttext_random', 'de', 'ru'),\n",
    "                    ('fasttext_top', 'de', 'ru'),\n",
    "                    #('fasttext_random', 'de', 'zh-CN'),\n",
    "                    ('fasttext_top', 'de', 'zh-CN'),\n",
    "                    #('fasttext_random', 'ru', 'en'),\n",
    "                    ('fasttext_top', 'ru', 'en'),\n",
    "                    #('fasttext_random', 'ru', 'es'),\n",
    "                    ('fasttext_top', 'ru', 'es'),\n",
    "                    #('fasttext_random', 'ru', 'zh-CN'),\n",
    "                    ('fasttext_top', 'ru', 'zh-CN'),\n",
    "                    #('fasttext_random', 'ru', 'de'),\n",
    "                    ('fasttext_top', 'ru', 'de'),\n",
    "                    #('fasttext_random', 'zh-CN', 'en'),\n",
    "                    ('fasttext_top', 'zh-CN', 'en'),\n",
    "                    #('fasttext_random', 'zh-CN', 'es'),\n",
    "                    ('fasttext_top', 'zh-CN', 'es'),\n",
    "                    #('fasttext_random', 'zh-CN', 'ru'),\n",
    "                    ('fasttext_top', 'zh-CN', 'ru'),\n",
    "                    #('fasttext_random', 'zh-CN', 'de'),\n",
    "                    ('fasttext_top', 'zh-CN', 'de'),\n",
    "                    #('fasttext_random', 'es', 'en'),\n",
    "                    ('fasttext_top', 'es', 'en'),\n",
    "                    #('fasttext_random', 'es', 'de'),\n",
    "                    ('fasttext_top', 'es', 'de'),\n",
    "                    #('fasttext_random', 'es', 'ru'),\n",
    "                    ('fasttext_top', 'es', 'ru'),\n",
    "                    #('fasttext_random', 'es', 'zh-CN'),\n",
    "                    ('fasttext_top', 'es', 'zh-CN')\n",
    "\n",
    "                    ]\n",
    "\n",
    "    md = ''\n",
    "    for translation in translations:\n",
    "        embedding, lg1, lg2 = translation\n",
    "        # Vocab/Vectors/Dicts\n",
    "        lg1_vocab, lg1_vectors, lg2_vocab, lg2_vectors = \\\n",
    "            pickle_rw((lg1 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg1 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      write=False)\n",
    "        lg1_dict = make_dict(lg1_vocab, lg1_vectors)\n",
    "        lg2_dict = make_dict(lg2_vocab, lg2_vectors)\n",
    "\n",
    "        print('Translation: '+lg1+'->'+lg2+'\\n')\n",
    "\n",
    "        # Train/Test Vocab/Vectors\n",
    "        vocab_train, vocab_test = vocab_train_test(embedding, lg1, lg2, lg1_vocab)\n",
    "        X_train, X_test, y_train, y_test = vectors_train_test(vocab_train,\n",
    "                                                              vocab_test)\n",
    "\n",
    "        # Fit tranlation matrix to training data\n",
    "        model, history, T, D, tf,I, M = translation_matrix(X_train, y_train)\n",
    "\n",
    "        print('biggest element:'+str(np.max(T))+'\\n')\n",
    "        \n",
    "        U,s,Vh = svd(T)\n",
    "        \n",
    "        #scaler = MinMaxScaler()\n",
    "        \n",
    "        #scaler.fit(s)\n",
    "        \n",
    "        #s = scaler.transform(s)\n",
    "        \n",
    "        print(\"min: \"+str(min(s))+\"\\n\")\n",
    "        \n",
    "        print(\"max: \"+str(max(s))+\"\\n\")\n",
    "        \n",
    "        print(\"mean: \"+str(np.mean(s))+\"\\n\")\n",
    "        \n",
    "        print(\"median: \"+str(np.median(s))+\"\\n\")\n",
    "        \n",
    "        print(\"std: \"+str(np.std(s))+\"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        s = np.log10(s)\n",
    "        \n",
    "        print(\"min: \"+str(min(s))+\"\\n\")\n",
    "        \n",
    "        print(\"max: \"+str(max(s))+\"\\n\")\n",
    "        \n",
    "        print(\"mean: \"+str(np.mean(s))+\"\\n\")\n",
    "        \n",
    "        print(\"median: \"+str(np.median(s))+\"\\n\")\n",
    "        \n",
    "        print(\"std: \"+str(np.std(s))+\"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(s)\n",
    "        \n",
    "        plt.hist(s,bins='auto')#bins=50,normed='True',range = (0.0,0.2))\n",
    "        #plt.plot(s)\n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: en->ru\n",
      "\n",
      "length of en-ru vocab: 6839\n",
      "Determinant:6.68409e-20\n",
      "Fr_norm:-0.0\n",
      "Inverse:[[  11.04030704  -58.05263901  -70.07519531 ...,   25.41628647\n",
      "    30.97108078  -40.67876816]\n",
      " [  91.81015015  155.81185913   63.31090164 ...,  -51.2548027   -34.41306305\n",
      "    47.90806961]\n",
      " [  23.46608734   12.1979866   -52.79821777 ...,   23.93603897\n",
      "    -7.66915035    5.51542711]\n",
      " ..., \n",
      " [ -95.40709686  -29.07827187   87.29953003 ...,  100.88738251  -16.7738266\n",
      "   153.37922668]\n",
      " [-147.87860107  -81.68314362  -44.3213768  ...,  173.213974    -59.09873962\n",
      "    59.42921829]\n",
      " [  16.56924438 -103.65172577 -142.10444641 ...,  -78.63152313\n",
      "   117.04570007 -168.28567505]]\n",
      "\n",
      "Translation: en->de\n",
      "\n",
      "length of en-de vocab: 16693\n",
      "Determinant:2.40427e-21\n",
      "Fr_norm:0.0\n",
      "Inverse:[[  755.14819336  -133.340271     206.62522888 ...,  1882.01904297\n",
      "   -709.5010376  -1083.3581543 ]\n",
      " [  242.50422668   -23.07047462     5.77775288 ...,   960.47021484\n",
      "   -322.10214233  -420.7321167 ]\n",
      " [ -431.28991699    97.79253387   -95.18783569 ..., -1025.21264648\n",
      "    330.7355957    584.13464355]\n",
      " ..., \n",
      " [ -422.92025757    46.88298035    96.03134155 ..., -1551.60412598\n",
      "    493.1998291    651.64038086]\n",
      " [  611.00964355  -180.95664978   202.88879395 ...,  1523.80834961\n",
      "   -622.27716064  -950.3012085 ]\n",
      " [ -259.61535645    10.06036949   -96.83132935 ...,  -761.62390137\n",
      "    190.62011719   434.51565552]]\n",
      "\n",
      "Translation: en->es\n",
      "\n",
      "length of en-es vocab: 10422\n",
      "Determinant:-1.16279e-23\n",
      "Fr_norm:0.0\n",
      "Inverse:[[ -98.68668365 -260.83862305 -587.78167725 ...,  379.01580811\n",
      "    57.62456131  647.65167236]\n",
      " [ -93.85202026   31.28058624  144.4175415  ...,  -42.15445328\n",
      "     3.08430982 -151.31071472]\n",
      " [ 284.58007812 -404.34399414 -712.90319824 ...,  552.19470215\n",
      "   -39.45856476  884.69006348]\n",
      " ..., \n",
      " [ 200.77197266 -179.4989624  -525.70465088 ...,  256.63024902\n",
      "   -38.14390945  414.31124878]\n",
      " [-100.97593689 -223.18373108 -465.73358154 ...,  310.7088623    63.51074219\n",
      "   427.6505127 ]\n",
      " [-123.85021973 -202.60536194 -432.22723389 ...,  316.81027222\n",
      "    15.20920467  519.00701904]]\n",
      "\n",
      "Translation: en->zh-CN\n",
      "\n",
      "length of en-zh-CN vocab: 1460\n",
      "Determinant:-0.0\n",
      "Fr_norm:-0.0\n",
      "Inverse:[[ 279.08325195  -50.90645218  174.09918213 ...,   90.67852783\n",
      "  -125.23526001  138.2858429 ]\n",
      " [ 282.04492188  -24.62430573  276.77432251 ...,  248.19404602\n",
      "   -95.89566803  -59.2610321 ]\n",
      " [  -3.75852799 -152.42700195   65.89362335 ...,  302.29708862\n",
      "  -105.84304047 -232.65254211]\n",
      " ..., \n",
      " [ 211.2144165  -100.81359863  124.05513    ...,  331.4234314  -124.95433044\n",
      "  -216.73834229]\n",
      " [-125.23327637  128.56265259   89.98539734 ...,  -53.60145569\n",
      "    78.57273102   57.74006271]\n",
      " [ 179.14727783  -62.25080872  120.82265472 ...,  -73.63300323\n",
      "   103.99213409   30.01247787]]\n",
      "\n",
      "Translation: de->en\n",
      "\n",
      "length of de-en vocab: 16693\n",
      "Determinant:-2.15217e-17\n",
      "Fr_norm:-0.0\n",
      "Inverse:[[   1.84647417  -55.90779495  -43.97912598 ...,   39.81010818\n",
      "   -39.84086227 -105.50863647]\n",
      " [  91.50335693  135.40769958  272.12155151 ...,  170.50970459\n",
      "   -79.33867645 -129.79850769]\n",
      " [  -1.02397203   67.86239624   -5.41641092 ...,   94.55872345\n",
      "  -108.43566895  -34.19537735]\n",
      " ..., \n",
      " [ -55.88882065  -73.20679474 -167.59153748 ...,  -99.45224762\n",
      "    50.38871765   40.59305191]\n",
      " [  71.21019745   31.15022278  143.98403931 ...,   29.07103348\n",
      "   -51.79080582  -67.52401733]\n",
      " [   1.01958323   -3.17144513   -0.39861795 ...,  -27.27108765   64.1984024\n",
      "    79.6170578 ]]\n",
      "\n",
      "Translation: de->es\n",
      "\n",
      "length of de-es vocab: 11456\n",
      "Determinant:4.61531e-23\n",
      "Fr_norm:0.0\n",
      "Inverse:[[-208.12774658  129.30685425  286.38415527 ...,  161.75546265\n",
      "   -65.80812836 -153.16976929]\n",
      " [  47.54762268  -11.62705421   95.54808807 ...,  122.13251495\n",
      "    95.22483063 -182.45480347]\n",
      " [ -68.93095398   29.17188644   10.01784801 ...,   80.02574921\n",
      "   -23.01441765  -80.31826782]\n",
      " ..., \n",
      " [ -60.41200256   46.29036331   83.87097168 ...,  -34.00827026\n",
      "  -115.82743073  -35.87575531]\n",
      " [-335.98999023   66.93195343  189.75874329 ...,   24.15080833\n",
      "  -275.91699219   87.48773193]\n",
      " [   1.96587789  -25.36977577  105.56586456 ...,  206.55760193\n",
      "    43.54348755 -296.44512939]]\n",
      "\n",
      "Translation: de->ru\n",
      "\n",
      "length of de-ru vocab: 5910\n",
      "Determinant:-1.09467e-19\n",
      "Fr_norm:0.0\n",
      "Inverse:[[-151.40771484  -30.88466072   77.30550385 ...,   38.58583069\n",
      "   128.15287781  -20.5597744 ]\n",
      " [ -38.43872452  -37.58361816  -96.61126709 ...,   37.91732788\n",
      "   100.82440948   20.59609604]\n",
      " [-176.77554321   23.42058182   27.32201767 ...,  -75.23674011\n",
      "   106.20679474 -273.70327759]\n",
      " ..., \n",
      " [-194.94673157   -2.72268939 -151.26535034 ..., -133.86204529\n",
      "   145.88725281 -384.5640564 ]\n",
      " [  70.48829651  -24.49956322  -85.98475647 ...,  -49.60044098\n",
      "   -56.12587738  -85.30728912]\n",
      " [-378.18856812    7.60398245  -20.77565765 ..., -214.16625977\n",
      "    72.85224152 -715.15490723]]\n",
      "\n",
      "Translation: de->zh-CN\n",
      "\n",
      "length of de-zh-CN vocab: 1820\n",
      "Determinant:0.0\n",
      "Fr_norm:-0.0\n",
      "Inverse:[[  182.77256775   -12.98318291     8.14419556 ...,   -44.43376541\n",
      "     78.37240601   -77.56101227]\n",
      " [   80.42079163  -243.8031311   -268.67929077 ...,   112.42276764\n",
      "     37.31299591  -114.50111389]\n",
      " [  780.19976807  -752.38586426  -606.61517334 ...,  -107.82647705\n",
      "    144.76489258  -893.77630615]\n",
      " ..., \n",
      " [ -113.08462524   269.40768433   195.55578613 ...,   -34.58014297\n",
      "   -143.97172546   110.33963776]\n",
      " [  214.31100464  -313.65765381   -16.46645927 ...,   -42.66078186\n",
      "    139.57568359  -233.36196899]\n",
      " [  653.07806396 -1090.84179688  -776.70593262 ...,    27.33714294\n",
      "    350.93087769  -820.984375  ]]\n",
      "\n",
      "Translation: ru->en\n",
      "\n",
      "length of ru-en vocab: 6839\n",
      "Determinant:-1.14019e-20\n",
      "Fr_norm:0.0\n",
      "Inverse:[[-234.50318909   22.7103157   468.10519409 ..., -142.00621033\n",
      "   224.19677734 -438.63296509]\n",
      " [  44.44205093 -110.10383606 -214.72457886 ...,   56.50108719\n",
      "  -131.72332764   84.38804626]\n",
      " [ 288.88677979 -277.25476074 -490.5708313  ...,  175.72525024\n",
      "  -196.50019836  467.42556763]\n",
      " ..., \n",
      " [-241.14767456  226.14082336  325.89718628 ...,  -45.59486771\n",
      "   226.55795288 -221.37988281]\n",
      " [  -8.15202427  145.6502533    33.30801773 ..., -147.27935791\n",
      "   -15.43331718 -206.29437256]\n",
      " [-225.9641571   250.87538147  396.88607788 ..., -133.33071899\n",
      "   170.97853088 -433.68011475]]\n",
      "\n",
      "Translation: ru->es\n",
      "\n",
      "length of ru-es vocab: 5870\n",
      "Determinant:1.06374e-22\n",
      "Fr_norm:-0.0\n",
      "Inverse:[[-138.00456238   -6.45686722   71.60140991 ...,  -36.70297241\n",
      "    58.82127762  154.65583801]\n",
      " [  49.72098923  -52.75326538  104.88195801 ...,   20.12860298\n",
      "   -74.70955658   40.26996613]\n",
      " [  -2.66938472 -147.09407043   98.62489319 ...,   85.64072418\n",
      "    23.54691696  125.16918945]\n",
      " ..., \n",
      " [-118.95442963   13.25885868   91.0585022  ...,  -67.21045685\n",
      "    62.11876678  115.23573303]\n",
      " [ -36.5912056    65.4972229    -3.35349679 ...,  -29.58674812   53.3549614\n",
      "   113.96194458]\n",
      " [ -71.00119781 -125.49362183  112.67459106 ...,   49.41052246\n",
      "    34.73974228  -78.90220642]]\n",
      "\n",
      "Translation: ru->zh-CN\n",
      "\n",
      "length of ru-zh-CN vocab: 1266\n",
      "Determinant:0.0\n",
      "Fr_norm:-0.0\n",
      "Inverse:[[ 123.87176514  101.6381073   194.61276245 ...,  -42.12559891\n",
      "    96.63561249  138.75669861]\n",
      " [-744.82489014 -704.24475098 -446.41647339 ..., -491.5687561   -63.25275421\n",
      "   222.48565674]\n",
      " [  20.97368813   34.40427399  192.9274292  ..., -135.43104553\n",
      "   119.09024811  102.5777359 ]\n",
      " ..., \n",
      " [ 195.69140625   27.52499962   45.47441101 ...,  102.32037354\n",
      "     3.24064541  -41.37684631]\n",
      " [ 245.32792664  146.08032227  202.41160583 ...,   92.10801697\n",
      "    32.16794586    6.01663494]\n",
      " [ 108.89143372  119.61774445  169.33660889 ...,  119.6232605    44.22268295\n",
      "    93.17635345]]\n",
      "\n",
      "Translation: ru->de\n",
      "\n",
      "length of ru-de vocab: 5910\n",
      "Determinant:-1.0791e-19\n",
      "Fr_norm:0.0\n",
      "Inverse:[[ 231.7795105   -53.77041626   35.14373016 ...,   41.9601593   -18.7463932\n",
      "   138.1625824 ]\n",
      " [-185.92315674 -348.37637329 -128.53903198 ...,   -3.67792106\n",
      "   113.00837708  -41.82150269]\n",
      " [ 274.20281982   89.54084778   52.89003372 ...,  130.93800354\n",
      "  -106.85772705  159.0308075 ]\n",
      " ..., \n",
      " [ 195.22572327 -163.60261536   73.5228653  ..., -102.59391785\n",
      "   -55.86291504   83.46475983]\n",
      " [  13.79369545  -42.29308701   -1.58617067 ...,   57.35632324\n",
      "   -16.52289391   40.29460144]\n",
      " [  70.41092682  278.4899292   115.18925476 ...,  110.06312561\n",
      "   -41.55050659   37.66139984]]\n",
      "\n",
      "Translation: zh-CN->en\n",
      "\n",
      "length of zh-CN-en vocab: 1460\n",
      "Determinant:-0.0\n",
      "Fr_norm:0.0\n",
      "Inverse:[[  750.95532227 -2677.73120117 -2790.56835938 ...,  -500.21502686\n",
      "   6477.76611328  -906.92370605]\n",
      " [ -856.35272217  2233.87646484  2544.17041016 ...,   560.26184082\n",
      "  -5371.02587891   496.52096558]\n",
      " [-1010.84552002  2835.54589844  2717.16064453 ...,   713.1763916\n",
      "  -6876.35498047   718.14050293]\n",
      " ..., \n",
      " [ -357.97131348   629.50219727   654.39550781 ...,   254.75791931\n",
      "  -1516.64099121   115.86322784]\n",
      " [  610.44421387 -1544.09875488 -1381.81567383 ...,  -359.84481812\n",
      "   3450.50390625  -447.62716675]\n",
      " [-1167.98925781  2801.60986328  2767.17675781 ...,   756.30444336\n",
      "  -6591.30371094   868.70123291]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: zh-CN->es\n",
      "\n",
      "length of zh-CN-es vocab: 1164\n",
      "Determinant:-0.0\n",
      "Fr_norm:0.0\n",
      "Inverse:[[  17.54487991   23.9071846   -69.86971283 ...,   89.22011566\n",
      "    67.72798157  108.96387482]\n",
      " [ -19.26591682  -58.13566971  -63.12947464 ...,   12.89937305  102.1413269\n",
      "    64.83384705]\n",
      " [  53.17620087 -105.45384216  -46.82474899 ...,   29.39961052\n",
      "   285.44174194   51.69458771]\n",
      " ..., \n",
      " [ -97.70675659 -172.0501709    11.54387856 ...,  -69.68674469\n",
      "   -87.34493256  -99.78017426]\n",
      " [ 132.74069214  -67.03653717 -117.35790253 ...,   83.51582336\n",
      "   427.69055176  193.97187805]\n",
      " [ -33.97097397  -28.83506012   35.82284927 ...,   70.83348083\n",
      "   -68.67223358   97.35033417]]\n",
      "\n",
      "Translation: zh-CN->ru\n",
      "\n",
      "length of zh-CN-ru vocab: 1266\n",
      "Determinant:-0.0\n",
      "Fr_norm:-0.0\n",
      "Inverse:[[ -37.29304123   18.05200195  120.03998566 ..., -134.6721344    97.88696289\n",
      "    -2.2412765 ]\n",
      " [  81.02592468   48.94137955 -144.85075378 ...,   -4.63170481\n",
      "   -69.93479919   10.25763702]\n",
      " [  61.70194244 -108.61361694  -82.55732727 ...,  -18.13778687\n",
      "    68.86730194   25.89509964]\n",
      " ..., \n",
      " [  39.08507919  -40.19970322  -63.80712128 ...,   24.37394714\n",
      "    10.56565571  -40.56039429]\n",
      " [ -98.20683289   39.0988884   -53.12154007 ...,   48.00292587\n",
      "   -98.26872253   -1.27546012]\n",
      " [  12.94785118  -89.91964722   41.29096603 ...,   84.5228653    60.32333374\n",
      "   -83.64056396]]\n",
      "\n",
      "Translation: zh-CN->de\n",
      "\n",
      "length of zh-CN-de vocab: 1820\n",
      "Determinant:-0.0\n",
      "Fr_norm:-0.0\n",
      "Inverse:[[-167.89735413 -169.32441711  262.71853638 ...,  -49.06270218\n",
      "  -218.04176331 -502.0098877 ]\n",
      " [-211.6706543   -78.65090179  313.97622681 ...,   24.95234871\n",
      "  -260.29180908 -415.22360229]\n",
      " [-234.07113647  170.86058044  -28.66379738 ...,   49.25692368\n",
      "   -20.01120186  285.25259399]\n",
      " ..., \n",
      " [ 909.95092773  -99.72676086 -110.36309052 ..., -330.12548828\n",
      "   526.26983643  551.90856934]\n",
      " [ 923.55871582 -237.44372559  -63.26832199 ..., -316.19677734\n",
      "   187.92474365   53.57709885]\n",
      " [  54.23075104 -414.93478394  291.21899414 ..., -158.23045349\n",
      "  -240.38543701 -549.38092041]]\n",
      "\n",
      "Translation: es->en\n",
      "\n",
      "length of es-en vocab: 10422\n",
      "Determinant:-8.86887e-23\n",
      "Fr_norm:0.0\n",
      "Inverse:[[  3.66905022e+01   5.63451538e+01  -4.43629608e+01 ...,   1.47553272e+01\n",
      "   -2.23721695e+01   1.92639114e+02]\n",
      " [ -2.38301963e-01  -8.82871704e+01   1.55858719e+02 ...,   6.18005409e+01\n",
      "    6.82027817e+01  -8.13018112e+01]\n",
      " [ -9.10500641e+01   5.23132744e+01  -4.51392250e+01 ...,   3.51174103e+02\n",
      "   -7.27205811e+01  -9.86932297e+01]\n",
      " ..., \n",
      " [  2.95722256e+01  -2.66521339e+01   1.52041245e+02 ...,  -9.97995734e-01\n",
      "    5.86561394e+01   1.26562370e+02]\n",
      " [ -2.66694126e+01   1.24087509e+02  -1.58618713e+02 ...,   1.02118347e+02\n",
      "    1.51513977e+02  -4.62004089e+02]\n",
      " [  3.96808014e+01   2.40333366e+01   3.03186607e+01 ...,  -7.37969131e+01\n",
      "    1.03059483e+01   2.31150391e+02]]\n",
      "\n",
      "Translation: es->de\n",
      "\n",
      "length of es-de vocab: 11456\n",
      "Determinant:-1.67781e-22\n",
      "Fr_norm:-0.0\n",
      "Inverse:[[-211.0153656  -109.92762756 -235.15696716 ..., -462.22366333\n",
      "  -298.97665405  -98.86308289]\n",
      " [  50.72947311   31.43056297    7.85100508 ...,   32.22510147\n",
      "    61.28936005  -22.80888176]\n",
      " [ 153.84313965  146.11160278  214.22206116 ...,  406.88845825\n",
      "   189.72543335   96.86721039]\n",
      " ..., \n",
      " [-216.62428284  -85.879776   -237.69984436 ..., -293.89746094 -273.0133667\n",
      "  -253.99163818]\n",
      " [-192.18464661  -32.1150589  -150.17906189 ..., -291.48083496\n",
      "  -243.19403076 -141.9500885 ]\n",
      " [  84.87358093  -22.03468704  134.11616516 ...,  305.0322876   173.19493103\n",
      "    64.18505859]]\n",
      "\n",
      "Translation: es->ru\n",
      "\n",
      "length of es-ru vocab: 5870\n",
      "Determinant:3.42516e-22\n",
      "Fr_norm:-0.0\n",
      "Inverse:[[ -36.88964081  -38.66049194  171.79954529 ...,  -23.12669563\n",
      "   -56.12367249    6.87218428]\n",
      " [  53.82025909  212.07670593  -42.72406769 ..., -787.01470947\n",
      "   112.25466919   61.76227188]\n",
      " [  -6.68660975  177.28474426   32.79327393 ..., -769.54364014\n",
      "   125.00292969   -8.67141724]\n",
      " ..., \n",
      " [  38.5022583   150.08508301  -30.97211075 ..., -298.6656189   152.74972534\n",
      "   -61.62663651]\n",
      " [  51.41888809  211.19586182  -60.89271164 ..., -822.25720215\n",
      "   239.45413208  -25.49222755]\n",
      " [  19.69354248  128.50161743  -26.33551598 ..., -596.45941162\n",
      "    98.39897156   11.74662304]]\n",
      "\n",
      "Translation: es->zh-CN\n",
      "\n",
      "length of es-zh-CN vocab: 1164\n",
      "Determinant:-0.0\n",
      "Fr_norm:-0.0\n",
      "Inverse:[[  1.54408991e-01  -7.29956970e+01  -5.20634460e+01 ...,  -1.46622116e+02\n",
      "   -1.52809097e+02  -3.85142250e+01]\n",
      " [  7.74740076e+00   3.60843201e+01  -7.10157700e+01 ...,  -4.23915291e+01\n",
      "   -9.08426361e+01   3.14684830e+01]\n",
      " [  1.35625973e+01  -5.47755051e+01  -1.08091764e+01 ...,  -7.55519513e-03\n",
      "   -1.25363716e+02   2.30807434e+02]\n",
      " ..., \n",
      " [  1.06153564e+02  -2.54409866e+02   1.49036392e+02 ...,  -4.00590363e+01\n",
      "   -2.20805008e+02   1.89460587e+02]\n",
      " [  2.15465317e+02  -2.01847336e+02   9.14659595e+00 ...,  -8.39445953e+01\n",
      "   -2.12597778e+02   1.59251205e+02]\n",
      " [ -4.10690651e+01   3.38989067e+01   4.17062912e+01 ...,   8.21285019e+01\n",
      "   -1.81661606e+02   2.83516327e+02]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Manually set list of translations (embedding, lg1, lg2)\n",
    "    translations = [#('fasttext_random', 'en', 'ru'),\n",
    "                    ('fasttext_top', 'en', 'ru'),\n",
    "                    #('fasttext_random', 'en', 'de'),\n",
    "                    ('fasttext_top', 'en', 'de'),\n",
    "                    #('fasttext_random', 'en', 'es'),\n",
    "                    ('fasttext_top', 'en', 'es'),\n",
    "                    #('fasttext_random', 'en', 'zh-CN'),\n",
    "                    ('fasttext_top', 'en', 'zh-CN'),\n",
    "                    #('fasttext_random', 'de', 'en'),\n",
    "                    ('fasttext_top', 'de', 'en'),\n",
    "                    #('fasttext_random', 'de', 'es'),\n",
    "                    ('fasttext_top', 'de', 'es'),\n",
    "                    #('fasttext_random', 'de', 'ru'),\n",
    "                    ('fasttext_top', 'de', 'ru'),\n",
    "                    #('fasttext_random', 'de', 'zh-CN'),\n",
    "                    ('fasttext_top', 'de', 'zh-CN'),\n",
    "                    #('fasttext_random', 'ru', 'en'),\n",
    "                    ('fasttext_top', 'ru', 'en'),\n",
    "                    #('fasttext_random', 'ru', 'es'),\n",
    "                    ('fasttext_top', 'ru', 'es'),\n",
    "                    #('fasttext_random', 'ru', 'zh-CN'),\n",
    "                    ('fasttext_top', 'ru', 'zh-CN'),\n",
    "                    #('fasttext_random', 'ru', 'de'),\n",
    "                    ('fasttext_top', 'ru', 'de'),\n",
    "                    #('fasttext_random', 'zh-CN', 'en'),\n",
    "                    ('fasttext_top', 'zh-CN', 'en'),\n",
    "                    #('fasttext_random', 'zh-CN', 'es'),\n",
    "                    ('fasttext_top', 'zh-CN', 'es'),\n",
    "                    #('fasttext_random', 'zh-CN', 'ru'),\n",
    "                    ('fasttext_top', 'zh-CN', 'ru'),\n",
    "                    #('fasttext_random', 'zh-CN', 'de'),\n",
    "                    ('fasttext_top', 'zh-CN', 'de'),\n",
    "                    #('fasttext_random', 'es', 'en'),\n",
    "                    ('fasttext_top', 'es', 'en'),\n",
    "                    #('fasttext_random', 'es', 'de'),\n",
    "                    ('fasttext_top', 'es', 'de'),\n",
    "                    #('fasttext_random', 'es', 'ru'),\n",
    "                    ('fasttext_top', 'es', 'ru'),\n",
    "                    #('fasttext_random', 'es', 'zh-CN'),\n",
    "                    ('fasttext_top', 'es', 'zh-CN')\n",
    "\n",
    "                    ]\n",
    "\n",
    "    md = ''\n",
    "    for translation in translations:\n",
    "        embedding, lg1, lg2 = translation\n",
    "        # Vocab/Vectors/Dicts\n",
    "        lg1_vocab, lg1_vectors, lg2_vocab, lg2_vectors = \\\n",
    "            pickle_rw((lg1 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg1 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      write=False)\n",
    "        lg1_dict = make_dict(lg1_vocab, lg1_vectors)\n",
    "        lg2_dict = make_dict(lg2_vocab, lg2_vectors)\n",
    "\n",
    "        print('Translation: '+lg1+'->'+lg2+'\\n')\n",
    "\n",
    "        # Train/Test Vocab/Vectors\n",
    "        vocab_train, vocab_test = vocab_train_test(embedding, lg1, lg2, lg1_vocab)\n",
    "        X_train, X_test, y_train, y_test = vectors_train_test(vocab_train,\n",
    "                                                              vocab_test)\n",
    "\n",
    "        # Fit tranlation matrix to training data\n",
    "        model, history, T, D, tf, I, M = translation_matrix(X_train, y_train)\n",
    "\n",
    "        print('Inverse:'+str(I)+'\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sanity Check on Determinants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: en->ru\n",
      "\n",
      "length of en-ru vocab: 6839\n",
      "Determinant:2.03775e-19\n",
      "[ 0.67061174  0.6873163   0.85272115 ...,  1.20578921  1.40582168\n",
      "  0.93844253]\n",
      "[[  4.34536341e-04  -1.01042946e-03   1.29217806e-04 ...,   2.51814048e-03\n",
      "    2.25269631e-03  -3.78022756e-04]\n",
      " [ -2.70378281e-04   1.11204310e-04   3.61257058e-04 ...,   1.47847389e-03\n",
      "    2.56023044e-03   3.06686998e-04]\n",
      " [ -7.82030053e-04  -2.08674697e-03  -3.18972440e-03 ...,   1.43284816e-03\n",
      "    8.56383296e-04  -5.04361931e-04]\n",
      " ..., \n",
      " [  3.68732936e-03   1.84817414e-03   2.45620660e-03 ...,   4.04990744e-03\n",
      "   -1.71988842e-03  -2.58150208e-03]\n",
      " [  9.60162492e-04   5.78932850e-05   2.89971940e-03 ...,   8.86483584e-04\n",
      "   -2.09363061e-03  -8.01478338e-04]\n",
      " [ -3.22616356e-03  -2.79117911e-03  -1.80032675e-03 ...,   1.22218125e-03\n",
      "   -1.76331308e-03  -7.65946519e-04]]\n",
      "Volume of yhat matrix: inf\n",
      "\n",
      "Translation: en->de\n",
      "\n",
      "length of en-de vocab: 16693\n",
      "Determinant:-1.29322e-21\n",
      "[ 0.70435685  0.65216172  0.62163788 ...,  1.12678242  1.32142627\n",
      "  1.23906744]\n",
      "[[  2.71139224e-03  -1.48496684e-03  -1.56725768e-03 ...,  -1.17054768e-03\n",
      "   -1.34262850e-03  -1.98837882e-03]\n",
      " [  2.86348537e-03  -2.10620440e-03   2.23037554e-03 ...,  -6.03577180e-04\n",
      "    9.56599833e-04  -2.53266189e-03]\n",
      " [  5.87912276e-03  -9.92000662e-03   6.33286545e-03 ...,   9.39513731e-04\n",
      "    5.11228718e-05  -7.09268823e-03]\n",
      " ..., \n",
      " [ -6.36017649e-03   7.78413843e-03  -8.55566189e-03 ...,   9.38213780e-04\n",
      "    1.07714045e-03   8.00844096e-03]\n",
      " [ -4.13319608e-03   5.01748919e-03  -3.29270447e-03 ...,   6.99573080e-04\n",
      "   -2.63066206e-04   3.48947360e-03]\n",
      " [  1.22031802e-03   4.18335851e-03  -2.35816627e-03 ...,   3.97602853e-05\n",
      "    9.73376096e-04   5.03597199e-04]]\n",
      "Volume of yhat matrix: inf\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-26e1c462676e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m                       \u001b[0;34m(\u001b[0m\u001b[0mlg2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_vocab'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                       \u001b[0;34m(\u001b[0m\u001b[0mlg2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_vectors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                       write=False)\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mlg1_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlg1_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg1_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mlg2_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlg2_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg2_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/SkymindLabsInternship/Isomantics/code/gensim_download.py\u001b[0m in \u001b[0;36mpickle_rw\u001b[0;34m(write, *tuples)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../pickle/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Manually set list of translations (embedding, lg1, lg2)\n",
    "    translations = [#('fasttext_random', 'en', 'ru'),\n",
    "                    ('fasttext_top', 'en', 'ru'),\n",
    "                    #('fasttext_random', 'en', 'de'),\n",
    "                    ('fasttext_top', 'en', 'de'),\n",
    "                    #('fasttext_random', 'en', 'es'),\n",
    "                    ('fasttext_top', 'en', 'es'),\n",
    "                    #('fasttext_random', 'en', 'zh-CN'),\n",
    "                    ('fasttext_top', 'en', 'zh-CN'),\n",
    "                    #('fasttext_random', 'de', 'en'),\n",
    "                    ('fasttext_top', 'de', 'en'),\n",
    "                    #('fasttext_random', 'de', 'es'),\n",
    "                    ('fasttext_top', 'de', 'es'),\n",
    "                    #('fasttext_random', 'de', 'ru'),\n",
    "                    ('fasttext_top', 'de', 'ru'),\n",
    "                    #('fasttext_random', 'de', 'zh-CN'),\n",
    "                    ('fasttext_top', 'de', 'zh-CN'),\n",
    "                    #('fasttext_random', 'ru', 'en'),\n",
    "                    ('fasttext_top', 'ru', 'en'),\n",
    "                    #('fasttext_random', 'ru', 'es'),\n",
    "                    ('fasttext_top', 'ru', 'es'),\n",
    "                    #('fasttext_random', 'ru', 'zh-CN'),\n",
    "                    ('fasttext_top', 'ru', 'zh-CN'),\n",
    "                    #('fasttext_random', 'ru', 'de'),\n",
    "                    ('fasttext_top', 'ru', 'de'),\n",
    "                    #('fasttext_random', 'zh-CN', 'en'),\n",
    "                    ('fasttext_top', 'zh-CN', 'en'),\n",
    "                    #('fasttext_random', 'zh-CN', 'es'),\n",
    "                    ('fasttext_top', 'zh-CN', 'es'),\n",
    "                    #('fasttext_random', 'zh-CN', 'ru'),\n",
    "                    ('fasttext_top', 'zh-CN', 'ru'),\n",
    "                    #('fasttext_random', 'zh-CN', 'de'),\n",
    "                    ('fasttext_top', 'zh-CN', 'de'),\n",
    "                    #('fasttext_random', 'es', 'en'),\n",
    "                    ('fasttext_top', 'es', 'en'),\n",
    "                    #('fasttext_random', 'es', 'de'),\n",
    "                    ('fasttext_top', 'es', 'de'),\n",
    "                    #('fasttext_random', 'es', 'ru'),\n",
    "                    ('fasttext_top', 'es', 'ru'),\n",
    "                    #('fasttext_random', 'es', 'zh-CN'),\n",
    "                    ('fasttext_top', 'es', 'zh-CN')\n",
    "\n",
    "                    ]\n",
    "\n",
    "    md = ''\n",
    "    for translation in translations:\n",
    "        embedding, lg1, lg2 = translation\n",
    "        # Vocab/Vectors/Dicts\n",
    "        lg1_vocab, lg1_vectors, lg2_vocab, lg2_vectors = \\\n",
    "            pickle_rw((lg1 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg1 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      write=False)\n",
    "        lg1_dict = make_dict(lg1_vocab, lg1_vectors)\n",
    "        lg2_dict = make_dict(lg2_vocab, lg2_vectors)\n",
    "\n",
    "        print('Translation: '+lg1+'->'+lg2+'\\n')\n",
    "\n",
    "        # Train/Test Vocab/Vectors\n",
    "        vocab_train, vocab_test = vocab_train_test(embedding, lg1, lg2, lg1_vocab)\n",
    "        X_train, X_test, y_train, y_test = vectors_train_test(vocab_train,\n",
    "                                                              vocab_test)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Fit tranlation matrix to training data\n",
    "        model, history, T, D, tf, I, M = translation_matrix(X_train, y_train)\n",
    "\n",
    "        \n",
    "        y_norm = translation_results(X_test, y_test, vocab_test, T,\n",
    "                                        lg2_vectors, lg2_vocab)\n",
    "        \n",
    "        \n",
    "        #print('Volume of X matrix: '+ np.prod(X_norm)+'\\n')\n",
    "        print(y_norm)\n",
    "        print(T)\n",
    "        print('Volume of yhat matrix: '+ str(np.prod(y_norm))+'\\n')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
