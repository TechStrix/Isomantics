{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Models w/ Different Lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -copyToLocal gs://isomantics/code/ /root/notebooks/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/10/12 07:14:29 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.1-hadoop2\n",
      "-rwx------   3 root root   23588804 2017-10-02 22:40 gs://isomantics/pickle/en_fasttext_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls gs://isomantics/pickle/en_fasttext_vocab.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.5/site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.5/site-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.5/site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.5/site-packages (from keras)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.5/site-packages (from matplotlib)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.5/site-packages (from matplotlib)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.5/site-packages (from matplotlib)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /opt/conda/lib/python3.5/site-packages (from matplotlib)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in /opt/conda/lib/python3.5/site-packages (from matplotlib)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.5/site-packages (from matplotlib)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied: python-dateutil>=2 in /opt/conda/lib/python3.5/site-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /opt/conda/lib/python3.5/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.5/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.5/site-packages (from python-dateutil>=2->pandas)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.5/site-packages (from tensorflow)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.5/site-packages (from tensorflow)\n",
      "Requirement already satisfied: tensorflow-tensorboard<0.2.0,>=0.1.0 in /opt/conda/lib/python3.5/site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.5/site-packages (from tensorflow)\n",
      "Requirement already satisfied: protobuf>=3.3.0 in /opt/conda/lib/python3.5/site-packages (from tensorflow)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.5/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /opt/conda/lib/python3.5/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Requirement already satisfied: html5lib==0.9999999 in /opt/conda/lib/python3.5/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Requirement already satisfied: bleach==1.5.0 in /opt/conda/lib/python3.5/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.5/site-packages (from protobuf>=3.3.0->tensorflow)\n",
      "Collecting pydrive\n",
      "Requirement already up-to-date: oauth2client>=4.0.0 in /opt/conda/lib/python3.5/site-packages (from pydrive)\n",
      "Requirement already up-to-date: PyYAML>=3.0 in /opt/conda/lib/python3.5/site-packages (from pydrive)\n",
      "Requirement already up-to-date: google-api-python-client>=1.2 in /opt/conda/lib/python3.5/site-packages (from pydrive)\n",
      "Requirement already up-to-date: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.5/site-packages (from oauth2client>=4.0.0->pydrive)\n",
      "Requirement already up-to-date: httplib2>=0.9.1 in /opt/conda/lib/python3.5/site-packages (from oauth2client>=4.0.0->pydrive)\n",
      "Requirement already up-to-date: six>=1.6.1 in /opt/conda/lib/python3.5/site-packages (from oauth2client>=4.0.0->pydrive)\n",
      "Requirement already up-to-date: pyasn1>=0.1.7 in /opt/conda/lib/python3.5/site-packages (from oauth2client>=4.0.0->pydrive)\n",
      "Requirement already up-to-date: rsa>=3.1.4 in /opt/conda/lib/python3.5/site-packages (from oauth2client>=4.0.0->pydrive)\n",
      "Requirement already up-to-date: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.5/site-packages (from google-api-python-client>=1.2->pydrive)\n",
      "Installing collected packages: pydrive\n",
      "Successfully installed pydrive-1.3.1\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.5/site-packages (from sklearn)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! pip install keras\n",
    "! pip install matplotlib\n",
    "! pip install pandas\n",
    "! pip install seaborn\n",
    "! pip install numpy\n",
    "! pip install tensorflow\n",
    "! pip install pydrive --upgrade\n",
    "! pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-009520053b00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-storage-2.14.2\n",
      "\u001b[31m  Could not find a version that satisfies the requirement google-cloud-storage-2.14.2 (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for google-cloud-storage-2.14.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade google-cloud-storage-2.14.2 -t /opt/conda/lib/python3.5/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud\n",
      "  Using cached google_cloud-0.27.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-datastore<1.3dev,>=1.2.0 (from google-cloud)\n",
      "  Using cached google_cloud_datastore-1.2.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-speech<0.29dev,>=0.28.0 (from google-cloud)\n",
      "  Using cached google_cloud_speech-0.28.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-storage<1.4dev,>=1.3.0 (from google-cloud)\n",
      "  Using cached google_cloud_storage-1.3.2-py2.py3-none-any.whl\n",
      "Collecting google-cloud-dns<0.27dev,>=0.26.0 (from google-cloud)\n",
      "  Using cached google_cloud_dns-0.26.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-spanner<0.27dev,>=0.26.0 (from google-cloud)\n",
      "  Using cached google_cloud_spanner-0.26.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-vision<0.27dev,>=0.26.0 (from google-cloud)\n",
      "  Using cached google_cloud_vision-0.26.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-core<0.27dev,>=0.26.0 (from google-cloud)\n",
      "  Using cached google_cloud_core-0.26.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-runtimeconfig<0.27dev,>=0.26.0 (from google-cloud)\n",
      "  Using cached google_cloud_runtimeconfig-0.26.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-language<0.28dev,>=0.27.0 (from google-cloud)\n",
      "  Using cached google_cloud_language-0.27.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-monitoring<0.27dev,>=0.26.0 (from google-cloud)\n",
      "  Using cached google_cloud_monitoring-0.26.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-videointelligence<0.26dev,>=0.25.0 (from google-cloud)\n",
      "  Using cached google_cloud_videointelligence-0.25.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-bigtable<0.27dev,>=0.26.0 (from google-cloud)\n",
      "  Using cached google_cloud_bigtable-0.26.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-logging<1.3dev,>=1.2.0 (from google-cloud)\n",
      "  Using cached google_cloud_logging-1.2.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-bigquery<0.27dev,>=0.26.0 (from google-cloud)\n",
      "  Using cached google_cloud_bigquery-0.26.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-resource-manager<0.27dev,>=0.26.0 (from google-cloud)\n",
      "  Using cached google_cloud_resource_manager-0.26.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-pubsub<0.28dev,>=0.27.0 (from google-cloud)\n",
      "  Using cached google_cloud_pubsub-0.27.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-translate<1.2dev,>=1.1.0 (from google-cloud)\n",
      "  Using cached google_cloud_translate-1.1.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-error-reporting<0.27dev,>=0.26.0 (from google-cloud)\n",
      "  Using cached google_cloud_error_reporting-0.26.0-py2.py3-none-any.whl\n",
      "Collecting google-gax<0.16dev,>=0.15.7 (from google-cloud-datastore<1.3dev,>=1.2.0->google-cloud)\n",
      "Collecting gapic-google-cloud-datastore-v1<0.16dev,>=0.15.0 (from google-cloud-datastore<1.3dev,>=1.2.0->google-cloud)\n",
      "Collecting googleapis-common-protos[grpc]<2.0dev,>=1.5.2 (from google-cloud-speech<0.29dev,>=0.28.0->google-cloud)\n",
      "Collecting google-resumable-media>=0.2.3 (from google-cloud-storage<1.4dev,>=1.3.0->google-cloud)\n",
      "  Using cached google_resumable_media-0.2.3-py2.py3-none-any.whl\n",
      "Collecting google-auth>=1.0.0 (from google-cloud-storage<1.4dev,>=1.3.0->google-cloud)\n",
      "  Using cached google_auth-1.1.1-py2.py3-none-any.whl\n",
      "Collecting requests>=2.18.0 (from google-cloud-storage<1.4dev,>=1.3.0->google-cloud)\n",
      "  Using cached requests-2.18.4-py2.py3-none-any.whl\n",
      "Collecting grpcio<2.0dev,>=1.2.0 (from google-cloud-spanner<0.27dev,>=0.26.0->google-cloud)\n",
      "  Using cached grpcio-1.6.3-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting gapic-google-cloud-spanner-v1<0.16dev,>=0.15.0 (from google-cloud-spanner<0.27dev,>=0.26.0->google-cloud)\n",
      "Collecting gapic-google-cloud-spanner-admin-instance-v1<0.16dev,>=0.15.0 (from google-cloud-spanner<0.27dev,>=0.26.0->google-cloud)\n",
      "Collecting gapic-google-cloud-spanner-admin-database-v1<0.16dev,>=0.15.0 (from google-cloud-spanner<0.27dev,>=0.26.0->google-cloud)\n",
      "Collecting protobuf>=3.0.0 (from google-cloud-core<0.27dev,>=0.26.0->google-cloud)\n",
      "  Using cached protobuf-3.4.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting six (from google-cloud-core<0.27dev,>=0.26.0->google-cloud)\n",
      "  Using cached six-1.11.0-py2.py3-none-any.whl\n",
      "Collecting tenacity<5.0.0dev,>=4.0.0 (from google-cloud-core<0.27dev,>=0.26.0->google-cloud)\n",
      "Collecting gapic-google-cloud-logging-v2<0.92dev,>=0.91.0 (from google-cloud-logging<1.3dev,>=1.2.0->google-cloud)\n",
      "Collecting gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0 (from google-cloud-pubsub<0.28dev,>=0.27.0->google-cloud)\n",
      "Collecting gapic-google-cloud-error-reporting-v1beta1<0.16dev,>=0.15.0 (from google-cloud-error-reporting<0.27dev,>=0.26.0->google-cloud)\n",
      "Collecting ply==3.8 (from google-gax<0.16dev,>=0.15.7->google-cloud-datastore<1.3dev,>=1.2.0->google-cloud)\n",
      "Collecting dill<0.3dev,>=0.2.5 (from google-gax<0.16dev,>=0.15.7->google-cloud-datastore<1.3dev,>=1.2.0->google-cloud)\n",
      "Collecting future<0.17dev,>=0.16.0 (from google-gax<0.16dev,>=0.15.7->google-cloud-datastore<1.3dev,>=1.2.0->google-cloud)\n",
      "Collecting proto-google-cloud-datastore-v1[grpc]<0.91dev,>=0.90.3 (from gapic-google-cloud-datastore-v1<0.16dev,>=0.15.0->google-cloud-datastore<1.3dev,>=1.2.0->google-cloud)\n",
      "Collecting oauth2client<4.0dev,>=2.0.0 (from gapic-google-cloud-datastore-v1<0.16dev,>=0.15.0->google-cloud-datastore<1.3dev,>=1.2.0->google-cloud)\n",
      "Collecting pyasn1-modules>=0.0.5 (from google-auth>=1.0.0->google-cloud-storage<1.4dev,>=1.3.0->google-cloud)\n",
      "  Using cached pyasn1_modules-0.1.5-py2.py3-none-any.whl\n",
      "Collecting rsa>=3.1.4 (from google-auth>=1.0.0->google-cloud-storage<1.4dev,>=1.3.0->google-cloud)\n",
      "  Using cached rsa-3.4.2-py2.py3-none-any.whl\n",
      "Collecting pyasn1>=0.1.7 (from google-auth>=1.0.0->google-cloud-storage<1.4dev,>=1.3.0->google-cloud)\n",
      "  Using cached pyasn1-0.3.7-py2.py3-none-any.whl\n",
      "Collecting cachetools>=2.0.0 (from google-auth>=1.0.0->google-cloud-storage<1.4dev,>=1.3.0->google-cloud)\n",
      "  Using cached cachetools-2.0.1-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.23,>=1.21.1 (from requests>=2.18.0->google-cloud-storage<1.4dev,>=1.3.0->google-cloud)\n",
      "  Using cached urllib3-1.22-py2.py3-none-any.whl\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests>=2.18.0->google-cloud-storage<1.4dev,>=1.3.0->google-cloud)\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.18.0->google-cloud-storage<1.4dev,>=1.3.0->google-cloud)\n",
      "  Using cached certifi-2017.7.27.1-py2.py3-none-any.whl\n",
      "Collecting idna<2.7,>=2.5 (from requests>=2.18.0->google-cloud-storage<1.4dev,>=1.3.0->google-cloud)\n",
      "  Using cached idna-2.6-py2.py3-none-any.whl\n",
      "Collecting proto-google-cloud-spanner-v1[grpc]<0.16dev,>=0.15.3 (from gapic-google-cloud-spanner-v1<0.16dev,>=0.15.0->google-cloud-spanner<0.27dev,>=0.26.0->google-cloud)\n",
      "Collecting proto-google-cloud-spanner-admin-instance-v1[grpc]<0.16dev,>=0.15.3 (from gapic-google-cloud-spanner-admin-instance-v1<0.16dev,>=0.15.0->google-cloud-spanner<0.27dev,>=0.26.0->google-cloud)\n",
      "Collecting grpc-google-iam-v1<0.12dev,>=0.11.1 (from gapic-google-cloud-spanner-admin-instance-v1<0.16dev,>=0.15.0->google-cloud-spanner<0.27dev,>=0.26.0->google-cloud)\n",
      "Collecting proto-google-cloud-spanner-admin-database-v1[grpc]<0.16dev,>=0.15.3 (from gapic-google-cloud-spanner-admin-database-v1<0.16dev,>=0.15.0->google-cloud-spanner<0.27dev,>=0.26.0->google-cloud)\n",
      "Collecting setuptools (from protobuf>=3.0.0->google-cloud-core<0.27dev,>=0.26.0->google-cloud)\n",
      "  Using cached setuptools-36.5.0-py2.py3-none-any.whl\n",
      "Collecting monotonic>=0.6 (from tenacity<5.0.0dev,>=4.0.0->google-cloud-core<0.27dev,>=0.26.0->google-cloud)\n",
      "  Using cached monotonic-1.3-py2.py3-none-any.whl\n",
      "Collecting proto-google-cloud-logging-v2[grpc]<0.92dev,>=0.91.3 (from gapic-google-cloud-logging-v2<0.92dev,>=0.91.0->google-cloud-logging<1.3dev,>=1.2.0->google-cloud)\n",
      "Collecting proto-google-cloud-pubsub-v1[grpc]<0.16dev,>=0.15.4 (from gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub<0.28dev,>=0.27.0->google-cloud)\n",
      "Collecting proto-google-cloud-error-reporting-v1beta1[grpc]<0.16dev,>=0.15.3 (from gapic-google-cloud-error-reporting-v1beta1<0.16dev,>=0.15.0->google-cloud-error-reporting<0.27dev,>=0.26.0->google-cloud)\n",
      "Collecting httplib2>=0.9.1 (from oauth2client<4.0dev,>=2.0.0->gapic-google-cloud-datastore-v1<0.16dev,>=0.15.0->google-cloud-datastore<1.3dev,>=1.2.0->google-cloud)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: six, setuptools, protobuf, grpcio, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, ply, dill, urllib3, chardet, certifi, idna, requests, future, googleapis-common-protos, google-gax, monotonic, tenacity, google-cloud-core, httplib2, oauth2client, proto-google-cloud-datastore-v1, gapic-google-cloud-datastore-v1, google-cloud-datastore, google-cloud-speech, google-resumable-media, google-cloud-storage, google-cloud-dns, proto-google-cloud-spanner-v1, gapic-google-cloud-spanner-v1, proto-google-cloud-spanner-admin-instance-v1, grpc-google-iam-v1, gapic-google-cloud-spanner-admin-instance-v1, proto-google-cloud-spanner-admin-database-v1, gapic-google-cloud-spanner-admin-database-v1, google-cloud-spanner, google-cloud-vision, google-cloud-runtimeconfig, google-cloud-language, google-cloud-monitoring, google-cloud-videointelligence, google-cloud-bigtable, proto-google-cloud-logging-v2, gapic-google-cloud-logging-v2, google-cloud-logging, google-cloud-bigquery, google-cloud-resource-manager, proto-google-cloud-pubsub-v1, gapic-google-cloud-pubsub-v1, google-cloud-pubsub, google-cloud-translate, proto-google-cloud-error-reporting-v1beta1, gapic-google-cloud-error-reporting-v1beta1, google-cloud-error-reporting, google-cloud\n",
      "Successfully installed cachetools-2.0.1 certifi-2017.7.27.1 chardet-3.0.4 dill-0.2.7.1 future-0.16.0 gapic-google-cloud-datastore-v1-0.15.3 gapic-google-cloud-error-reporting-v1beta1-0.15.3 gapic-google-cloud-logging-v2-0.91.3 gapic-google-cloud-pubsub-v1-0.15.4 gapic-google-cloud-spanner-admin-database-v1-0.15.3 gapic-google-cloud-spanner-admin-instance-v1-0.15.3 gapic-google-cloud-spanner-v1-0.15.3 google-auth-1.1.1 google-cloud-0.27.0 google-cloud-bigquery-0.26.0 google-cloud-bigtable-0.26.0 google-cloud-core-0.26.0 google-cloud-datastore-1.2.0 google-cloud-dns-0.26.0 google-cloud-error-reporting-0.26.0 google-cloud-language-0.27.0 google-cloud-logging-1.2.0 google-cloud-monitoring-0.26.0 google-cloud-pubsub-0.27.0 google-cloud-resource-manager-0.26.0 google-cloud-runtimeconfig-0.26.0 google-cloud-spanner-0.26.0 google-cloud-speech-0.28.0 google-cloud-storage-1.3.2 google-cloud-translate-1.1.0 google-cloud-videointelligence-0.25.0 google-cloud-vision-0.26.0 google-gax-0.15.15 google-resumable-media-0.2.3 googleapis-common-protos-1.5.3 grpc-google-iam-v1-0.11.4 grpcio-1.6.3 httplib2-0.10.3 idna-2.6 monotonic-1.3 oauth2client-3.0.0 ply-3.8 proto-google-cloud-datastore-v1-0.90.4 proto-google-cloud-error-reporting-v1beta1-0.15.3 proto-google-cloud-logging-v2-0.91.3 proto-google-cloud-pubsub-v1-0.15.4 proto-google-cloud-spanner-admin-database-v1-0.15.3 proto-google-cloud-spanner-admin-instance-v1-0.15.3 proto-google-cloud-spanner-v1-0.15.3 protobuf-3.4.0 pyasn1-0.3.7 pyasn1-modules-0.1.5 requests-2.18.4 rsa-3.4.2 setuptools-36.5.0 six-1.11.0 tenacity-4.4.0 urllib3-1.22\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade google-cloud -t /opt/conda/lib/python3.5/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'scopes_to_string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-96cce6533727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnregularizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim_download\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle_rw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/code/gensim_download.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleDrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/pydrive/auth.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moauth2client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientRedirectHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moauth2client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientRedirectServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moauth2client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscopes_to_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapiattr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mApiAttribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapiattr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mApiAttributeMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'scopes_to_string'"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import nregularizer\n",
    "import numpy as np\n",
    "from gensim_download import pickle_rw\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "# import plotly.plotly as py\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy.linalg\n",
    "\n",
    "from numpy.linalg import det\n",
    "\n",
    "from numpy.linalg import inv\n",
    "\n",
    "#import sympy as sympy\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "#import cloudstorage\n",
    "\n",
    "#from sympy import Matrix\n",
    "# from plotly import __version__\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dict(vocab, vectors):\n",
    "    \"\"\"Make dictionary of vocab and vectors\"\"\"\n",
    "    return {vocab[i]: vectors[i] for i in range(len(vocab))}\n",
    "\n",
    "\n",
    "def vocab_train_test(embedding, lg1, lg2, lg1_vocab):\n",
    "    \"\"\"Create training and test vocabularies\"\"\"\n",
    "    if embedding == 'zeroshot':\n",
    "        with open('../data/zeroshot/transmat/data/' +\n",
    "                  'OPUS_en_it_europarl_train_5K.txt') as f:\n",
    "            vocab_train = [(_.split(' ')[0], _.split(' ')[1])\n",
    "                           for _ in f.read().split('\\n')[:-1]]\n",
    "        with open('../data/zeroshot/transmat/data/' +\n",
    "                  'OPUS_en_it_europarl_test.txt') as f:\n",
    "            vocab_test = [(_.split(' ')[0], _.split(' ')[1])\n",
    "                          for _ in f.read().split('\\n')[:-1]]\n",
    "\n",
    "    elif embedding in ['fasttext_random', 'fasttext_top']:\n",
    "        embedding, split = embedding.split('_')\n",
    "        lg1_lg2, lg2_lg1 = pickle_rw((lg1 + '_' + lg2, 0),\n",
    "                                     (lg2 + '_' + lg1, 0), write=False)\n",
    "        # T = Translation, R = Reverse (translated and then translated back)\n",
    "        # Create vocab from 2D translations\n",
    "        vocab_2D = []\n",
    "        for lg1_word in lg1_vocab:\n",
    "            # Translate lg1_word\n",
    "            if lg1_word in lg1_lg2:\n",
    "                lg1_word_T = lg1_lg2[lg1_word]\n",
    "\n",
    "                # Check if translated word (or lowercase) is in lg2_lg1\n",
    "                if lg1_word_T in lg2_lg1.keys():\n",
    "                    lg1_word_R = lg2_lg1[lg1_word_T]\n",
    "                elif lg1_word_T.lower() in lg2_lg1.keys():\n",
    "                    lg1_word_T = lg1_word_T.lower()\n",
    "                    lg1_word_R = lg2_lg1[lg1_word_T]\n",
    "                else:\n",
    "                    lg1_word_R = None\n",
    "\n",
    "                # Check if lg1_word and lg1_word_R are equal (lowercase)\n",
    "                if lg1_word_R:\n",
    "                    if lg1_word.lower() == lg1_word_R.lower():\n",
    "                        vocab_2D.append((lg1_word, lg1_word_T))\n",
    "        print('length of '+ lg1+'-'+ lg2+ ' vocab: '+str(len(vocab_2D)))\n",
    "\n",
    "        #Create Train/Test vocab\n",
    "\n",
    "        if split == 'random':\n",
    "            sample = np.random.choice(len(vocab_2D), 6500, replace=False)\n",
    "            vocab_train = np.asarray(vocab_2D)[sample[:5000]].tolist()\n",
    "            vocab_test = np.asarray(vocab_2D)[sample[5000:]].tolist()\n",
    "        elif split == 'top':\n",
    "#             sample = np.random.choice(range(6500), 6500, replace=False)\n",
    "#             vocab_train = np.asarray(vocab_2D)[:5000, :].tolist()\n",
    "#             vocab_test = np.asarray(vocab_2D)[:1500, :].tolist()\n",
    "            \n",
    "#             sample = np.random.choice(range(6500), 6500, replace=False)\n",
    "            vocab_train = np.asarray(vocab_2D)[:700, :].tolist()\n",
    "            vocab_test = np.asarray(vocab_2D)[:200, :].tolist()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # if split == 'random':\n",
    "        #     sample = np.random.choice(len(vocab_2D), 900, replace=False)\n",
    "        #     vocab_train = np.asarray(vocab_2D)[sample[:700]].tolist()\n",
    "        #     vocab_test = np.asarray(vocab_2D)[sample[700:]].tolist()\n",
    "        # elif split == 'top':\n",
    "        #     sample = np.random.choice(range(900), 900, replace=False)\n",
    "        #     vocab_train = np.asarray(vocab_2D)[:700, :].tolist()\n",
    "        #     vocab_test = np.asarray(vocab_2D)[:200, :].tolist()\n",
    "        # else:\n",
    "        #     pass\n",
    "\n",
    "    return vocab_train, vocab_test\n",
    "\n",
    "\n",
    "def vectors_train_test(vocab_train, vocab_test):\n",
    "    \"\"\"Create training and test vectors\"\"\"\n",
    "    X_train, y_train = zip(*[(lg1_dict[lg1_word], lg2_dict[lg2_word])\n",
    "                             for lg1_word, lg2_word in vocab_train])\n",
    "    X_test, y_test = zip(*[(lg1_dict[lg1_word], lg2_dict[lg2_word])\n",
    "                           for lg1_word, lg2_word in vocab_test])\n",
    "    return map(np.asarray, (X_train, X_test, y_train, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def translation_matrix(X_train, y_train, l):\n",
    "    \"\"\"Fit translation matrix T\"\"\"\n",
    "    #def norm_reg(weight_matrix):\n",
    "     #   return 0.01 * np.linalg.norm(np.matrix(np.subtract(np.matmul(weight_matrix,weight_matrix.T,np.matmul(weight_matrix.T,weight_matrix))),'fro')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, use_bias=False, input_shape=(X_train.shape[1],),kernel_regularizer=nregularizer.l3(l)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    history = model.fit(X_train, y_train, batch_size=128, epochs=20,\n",
    "                        verbose=False)\n",
    "    T = model.get_weights()[0]\n",
    "    \n",
    "    M = np.matrix(T)\n",
    "\n",
    "    Fr_norm = np.linalg.norm(np.matrix(np.subtract(np.matmul(M,np.transpose(M)),np.matmul(np.transpose(M),M))),'fro')\n",
    "\n",
    "    #print (\"Determinant:\"+str(D))\n",
    "    \n",
    "    #print (\"Fr_norm:\"+str(Fr_norm))\n",
    "\n",
    "    return T, Fr_norm \n",
    "\n",
    "def translation_accuracy(X_test, y_test, T):\n",
    "    \"\"\"Get predicted matrix 'yhat' using 'T' and find translation accuracy\"\"\"\n",
    "    # yhat\n",
    "    yhat = X_test.dot(T)\n",
    "    count = 0\n",
    "    for i in range(len(y_test)):\n",
    "        #if yhat[i,:].all() == y_test[i,:].all():\n",
    "        if np.array_equal(yhat[i,:],y_test[i,:]) == True:\n",
    "            count = count + 1\n",
    "    accuracy = count/len(y_test)*100\n",
    "    return accuracy\n",
    "\n",
    "def svd(T):\n",
    "    \"\"\"Perform SVD on the translation matrix 'T' \"\"\"\n",
    "    U, s, Vh = numpy.linalg.svd(T, full_matrices=False )\n",
    "    return U, s, Vh\n",
    "\n",
    "def T_svd_EDA(s):\n",
    "    \"\"\"Perform SVD on the translation matrix 'T' \"\"\"\n",
    "    plt.hist(s, bins='auto', range = (0,1),normed = 1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize(matrix):\n",
    "    \"\"\"Normalize the rows of a matrix\"\"\"\n",
    "    matrix_norm = np.linalg.norm(matrix, axis=1)\n",
    "    matrix_normed = matrix / np.repeat(matrix_norm, matrix.shape[1]). \\\n",
    "        reshape(matrix.shape)\n",
    "    return matrix_norm, matrix_normed\n",
    "\n",
    "\n",
    "def translation_results(X, y, vocab, T, lg2_vectors, lg2_vocab):\n",
    "    \"\"\"X, y, vocab - The training or test data that you want results for\n",
    "    T - The translation matrix\n",
    "    lg2_vectors, lg2_vocab - Foreign language used to find the nearest neighbor\n",
    "    \"\"\"\n",
    "\n",
    "    # Data Prep on Inputs\n",
    "    X_word, y_word = zip(*vocab)\n",
    "    X_norm, X_normed = normalize(X)\n",
    "    y_norm, y_normed = normalize(y)\n",
    "    lg2_vectors_norm, lg2_vectors_normed = normalize(lg2_vectors)\n",
    "\n",
    "    # yhat\n",
    "    yhat = X.dot(T)\n",
    "    yhat_norm, yhat_normed = normalize(yhat)\n",
    "\n",
    "    #X_norm = normalize(X)\n",
    "\n",
    "\n",
    "    # Nearest Neighbors\n",
    "    neg_cosine = -yhat_normed.dot(lg2_vectors_normed.T)\n",
    "    ranked_neighbor_indices = np.argsort(neg_cosine, axis=1)\n",
    "\n",
    "    # Nearest Neighbor\n",
    "    nearest_neighbor_indices = ranked_neighbor_indices[:, 0]\n",
    "    yhat_neighbor = lg2_vectors[nearest_neighbor_indices, :]\n",
    "    yhat_neighbor_norm, yhat_neighbor_normed = normalize(yhat_neighbor)\n",
    "    yhat_neighbor_word = np.asarray(lg2_vocab)[nearest_neighbor_indices]\n",
    "\n",
    "    # Results DF\n",
    "    cols = ['X_norm', 'y_norm', 'yhat_norm', 'yhat_neighbor_norm',\n",
    "            'X_word', 'y_word', 'yhat_neighbor_word']\n",
    "    results_df = pd.DataFrame({'X_norm': X_norm,\n",
    "                               'y_norm': y_norm,\n",
    "                               'yhat_norm': yhat_norm,\n",
    "                               'yhat_neighbor_norm': yhat_neighbor_norm,\n",
    "                               'X_word': X_word,\n",
    "                               'y_word': y_word,\n",
    "                               'yhat_neighbor_word': yhat_neighbor_word,})\n",
    "    results_df = results_df[cols]\n",
    "    results_df['neighbor_correct'] = results_df.y_word == \\\n",
    "        results_df.yhat_neighbor_word\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def T_norm_EDA(results_df):\n",
    "    \"\"\"Plot result norms side-by-side\"\"\"\n",
    "    test_size = results_df.shape[0]\n",
    "    test_accuracy = results_df.neighbor_correct.mean()\n",
    "\n",
    "    #print('Test Accuracy: '+str(test_accuracy)+'\\n')\n",
    "\n",
    "    #plot_data = ['X_norm', 'y_norm', 'yhat_norm', 'yhat_neighbor_norm']\n",
    "    # f, ax = plt.subplots(len(plot_data), sharex=True, sharey=True,\n",
    "    #                      figsize=(10, 10))\n",
    "    # for i, d in enumerate(plot_data):\n",
    "    #     ax[i].hist(results_df[d], bins=100)\n",
    "    #     ax[i].axis('off')\n",
    "    #     title = '{}: mean={}, std={}'.format(d, round(results_df[d].mean(), 2), round(results_df[d].std(), 2))\n",
    "    #     ax[i].set_title(title)\n",
    "    # f.subplots_adjust(hspace=0.7)\n",
    "    # plt.savefig('../images/' + lg1 + '_' + lg2 + '_' + embedding +\n",
    "    #             '_T_norm.png')\n",
    "    # plt.close('all')\n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "def T_pca_EDA(T):\n",
    "    \"\"\"PCA on matrix T\"\"\"\n",
    "    T_ss = StandardScaler().fit_transform(T)\n",
    "    pca = PCA().fit(T_ss)\n",
    "    n = pca.n_components_\n",
    "\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.xlim((0, n))\n",
    "    # plt.ylim((0, 1))\n",
    "    # plt.plot(range(n + 1), [0] + np.cumsum(pca.explained_variance_ratio_).\n",
    "    #          tolist())\n",
    "    # plt.plot(range(n + 1), np.asarray(range(n + 1)) / n)\n",
    "    # plt.xlabel('Number of Eigenvectors')\n",
    "    # plt.ylabel('Explained Variance')\n",
    "    # plt.savefig('../images/' + lg1 + '_' + lg2 + '_' + embedding +\n",
    "    #             '_T_isotropy.png')\n",
    "    # plt.close('all')\n",
    "\n",
    "    isotropy = (1 - sum(np.cumsum(pca.explained_variance_ratio_) * 1 / n)) / .5\n",
    "    return isotropy\n",
    "\n",
    "\n",
    "def T_report_results(embedding, lg1, lg2, lg1_vectors, lg2_vectors,\n",
    "                     X_train, X_test, D, results_df, isotropy):\n",
    "    md = '## ' + lg1.title() + ' to ' + lg2.title() + ' ' + \\\n",
    "        embedding.title() + '  \\n'\n",
    "    md += '- ' + lg1.title() + ' Vocabulary Size = ' + \\\n",
    "        '{:,.0f}'.format(lg1_vectors.shape[0]) + '  \\n'\n",
    "    md += '- ' + lg1.title() + ' Embedding Length = ' + \\\n",
    "        '{:,.0f}'.format(lg1_vectors.shape[1]) + '  \\n'\n",
    "    md += '- ' + lg2.title() + ' Vocabulary Size = ' + \\\n",
    "        '{:,.0f}'.format(lg2_vectors.shape[0]) + '  \\n'\n",
    "    md += '- ' + lg2.title() + ' Embedding Length = ' + \\\n",
    "        '{:,.0f}'.format(lg2_vectors.shape[1]) + '  \\n'\n",
    "    md += '- Train Size = ' + '{:,.0f}'.format(X_train.shape[0]) + '  \\n'\n",
    "    md += '- Test Size = ' + '{:,.0f}'.format(X_test.shape[0]) + '  \\n'\n",
    "    md += '- Determinant = ' + '{:,.0f}'.format(D) + '  \\n'\n",
    "\n",
    "    md += '- <b>Test Accuracy = ' + \\\n",
    "        '{:,.1%}'.format(results_df.neighbor_correct.mean()) + '</b>  \\n\\n'\n",
    "\n",
    "\n",
    "\n",
    "    md += '#### Test L2 Norms  \\n'\n",
    "    md += '- X_norm: L2 norms for ' + lg1.title() + ' test vectors  \\n'\n",
    "    md += '- y_norm: L2 norms for ' + lg2.title() + ' test vectors  \\n'\n",
    "    md += '- yhat_norm: L2 norms for X.dot(T) test vectors ' + \\\n",
    "        '(T = translation matrix)  \\n'\n",
    "    md += '- yhat_neighbor norm: L2 norms for nearest neighbor' + \\\n",
    "        'to X.dot(T) in y test vectors  \\n'\n",
    "    md += '![](../images/' + lg1 + '_' + lg2 + '_' + embedding + \\\n",
    "        '_T_norm.png)  \\n\\n'\n",
    "\n",
    "    md += '#### Translation Matrix Isotropy  \\n'\n",
    "    md += '- Isotropy = ' + '{:,.1%}'.format(isotropy) + '  \\n'\n",
    "    md += '![](../images/' + lg1 + '_' + lg2 + '_' + embedding + \\\n",
    "        '_T_isotropy.png)  \\n\\n'\n",
    "\n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gs:///pickle/en_fasttext_vocab.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-e4f2ccc57ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m                       \u001b[0;34m(\u001b[0m\u001b[0mlg2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_vocab'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                       \u001b[0;34m(\u001b[0m\u001b[0mlg2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_vectors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                       write=False)\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mlg1_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlg1_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg1_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mlg2_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlg2_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg2_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/gensim_download.py\u001b[0m in \u001b[0;36mpickle_rw\u001b[0;34m(write, *tuples)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://isomantics/pickle/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gs:///pickle/en_fasttext_vocab.pkl'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Manually set list of translations (embedding, lg1, lg2)\n",
    "    translations = [#('fasttext_random', 'en', 'ru'),\n",
    "                    ('fasttext_top', 'en', 'en'),\n",
    "                    ('fasttext_top', 'en', 'ru'),\n",
    "                    #('fasttext_random', 'en', 'de'),\n",
    "                    ('fasttext_top', 'en', 'de'),\n",
    "                    #('fasttext_random', 'en', 'es'),\n",
    "                    ('fasttext_top', 'en', 'es'),\n",
    "                    #('fasttext_random', 'en', 'zh-CN'),\n",
    "                    ('fasttext_top', 'en', 'zh-CN'),\n",
    "        \n",
    "        \n",
    "        \n",
    "                     #('fasttext_random', 'ru', 'en'),\n",
    "                    ('fasttext_top', 'ru', 'en'),\n",
    "                    #('fasttext_random', 'ru', 'es'),\n",
    "                    ('fasttext_top', 'ru', 'ru'),\n",
    "                    ('fasttext_top', 'ru', 'de'),\n",
    "                    ('fasttext_top', 'ru', 'es'),\n",
    "                    #('fasttext_random', 'ru', 'zh-CN'),\n",
    "                    ('fasttext_top', 'ru', 'zh-CN'),\n",
    "                    #('fasttext_random', 'ru', 'de'),\n",
    "                    \n",
    "        \n",
    "        \n",
    "                    #('fasttext_random', 'de', 'en'),\n",
    "                    ('fasttext_top', 'de', 'en'),\n",
    "                    #('fasttext_random', 'de', 'es'),\n",
    "                    ('fasttext_top', 'de', 'ru'), \n",
    "                    #('fasttext_random', 'de', 'ru'),\n",
    "                    ('fasttext_top', 'de', 'de'),\n",
    "                    ('fasttext_top', 'de', 'es'),\n",
    "                    #('fasttext_random', 'de', 'zh-CN'),\n",
    "                    ('fasttext_top', 'de', 'zh-CN'),\n",
    "        \n",
    "        \n",
    "        \n",
    "                    #('fasttext_random', 'es', 'en'),\n",
    "                    ('fasttext_top', 'es', 'en'),\n",
    "                    #('fasttext_random', 'es', 'de'),\n",
    "                    ('fasttext_top', 'es', 'ru'),\n",
    "                    ('fasttext_top', 'es', 'de'),\n",
    "                    #('fasttext_random', 'es', 'ru'),\n",
    "                    ('fasttext_top', 'es', 'es'),\n",
    "                    #('fasttext_random', 'es', 'zh-CN'),\n",
    "                    ('fasttext_top', 'es', 'zh-CN'),\n",
    "        \n",
    "                \n",
    "        \n",
    "                    #('fasttext_random', 'zh-CN', 'en'),\n",
    "                    ('fasttext_top', 'zh-CN', 'en'),\n",
    "                    #('fasttext_random', 'zh-CN', 'es'),\n",
    "                    ('fasttext_top', 'zh-CN', 'ru'),\n",
    "                    ('fasttext_top', 'zh-CN', 'de'),\n",
    "                    ('fasttext_top', 'zh-CN', 'es'),\n",
    "                    #('fasttext_random', 'zh-CN', 'ru'),\n",
    "                    ('fasttext_top', 'zh-CN', 'zh-CN'),\n",
    "                    #('fasttext_random', 'zh-CN', 'de'),\n",
    "        \n",
    "                    \n",
    "                    \n",
    "\n",
    "                    ]\n",
    "   \n",
    "    md = ''\n",
    "    for translation in translations:\n",
    "        embedding, lg1, lg2 = translation\n",
    "        # Vocab/Vectors/Dicts\n",
    "        lg1_vocab, lg1_vectors, lg2_vocab, lg2_vectors = \\\n",
    "            pickle_rw((lg1 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg1 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      write=False)\n",
    "        lg1_dict = make_dict(lg1_vocab, lg1_vectors)\n",
    "        lg2_dict = make_dict(lg2_vocab, lg2_vectors)\n",
    "\n",
    "        print('Translation: '+lg1+'->'+lg2+'\\n')\n",
    "\n",
    "        # Train/Test Vocab/Vectors\n",
    "        vocab_train, vocab_test = vocab_train_test(embedding, lg1, lg2, lg1_vocab)\n",
    "        X_train, X_test, y_train, y_test = vectors_train_test(vocab_train,\n",
    "                                                              vocab_test)\n",
    " \n",
    "        \n",
    "        # Fit tranlation matrix to training data\n",
    "        \n",
    "        T, fr_norm = translation_matrix(X_train, y_train, 1) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        T1, fr_norm1 = translation_matrix(X_train, y_train, 0.1)\n",
    "        \n",
    "        T01, fr_norm01 = translation_matrix(X_train, y_train, 0.01)\n",
    "        \n",
    "        T001, fr_norm001 = translation_matrix(X_train, y_train, 0.001)\n",
    "        \n",
    "        T0001, fr_norm0001 = translation_matrix(X_train, y_train, 0.0001)\n",
    "        \n",
    "        T00001, fr_norm00001 = translation_matrix(X_train, y_train, 0.00001)\n",
    "        \n",
    "        # Test Data Results\n",
    "        \n",
    "        results_df = translation_results(X_test, y_test, vocab_test, T,\n",
    "                                         lg2_vectors, lg2_vocab)\n",
    "        test_accuracy = T_norm_EDA(results_df)\n",
    "        \n",
    "#         test_accuracy = translation_accuracy(X_test, y_test, T)\n",
    "        \n",
    "        print(test_accuracy)\n",
    "        \n",
    "        \n",
    "        \n",
    "        results_df1 = translation_results(X_test, y_test, vocab_test, T1,\n",
    "                                         lg2_vectors, lg2_vocab)\n",
    "        test_accuracy1 = T_norm_EDA(results_df1)\n",
    "\n",
    "#         test_accuracy = translation_accuracy(X_test, y_test, T1)\n",
    "        \n",
    "        print(test_accuracy1)\n",
    "        \n",
    "        \n",
    "        results_df01 = translation_results(X_test, y_test, vocab_test, T01,\n",
    "                                         lg2_vectors, lg2_vocab)\n",
    "        test_accuracy01 = T_norm_EDA(results_df01)\n",
    "        \n",
    "        print(test_accuracy01)\n",
    "        \n",
    "        \n",
    "        results_df001 = translation_results(X_test, y_test, vocab_test, T001,\n",
    "                                         lg2_vectors, lg2_vocab)\n",
    "        test_accuracy001 = T_norm_EDA(results_df001)\n",
    "        \n",
    "        print(test_accuracy001)\n",
    "        \n",
    "        \n",
    "        results_df001 = translation_results(X_test, y_test, vocab_test, T001,\n",
    "                                         lg2_vectors, lg2_vocab)\n",
    "        test_accuracy001 = T_norm_EDA(results_df001)\n",
    "        \n",
    "        \n",
    "        results_df0001 = translation_results(X_test, y_test, vocab_test, T0001,\n",
    "                                         lg2_vectors, lg2_vocab)\n",
    "        test_accuracy0001 = T_norm_EDA(results_df0001)\n",
    "        \n",
    "        \n",
    "        results_df00001 = translation_results(X_test, y_test, vocab_test, T00001,\n",
    "                                         lg2_vectors, lg2_vocab)\n",
    "        test_accuracy00001 = T_norm_EDA(results_df00001)\n",
    "        \n",
    "        \n",
    "        #Plot the line graphs\n",
    "        \n",
    "        \n",
    "        Org_Acc = [test_accuracy ,test_accuracy ,test_accuracy ,test_accuracy, test_accuracy]\n",
    "        \n",
    "        New_Acc = []\n",
    "        New_Acc.append(test_accuracy1)\n",
    "        New_Acc.append(test_accuracy01)\n",
    "        New_Acc.append(test_accuracy001)\n",
    "        New_Acc.append(test_accuracy0001)\n",
    "        New_Acc.append(test_accuracy00001)\n",
    "        \n",
    "        \n",
    "        #New_Acc = [test_accuracy1 ,test_accuracy01 ,test_accuracy001 ,test_accuracy0001, test_accuracy00001]\n",
    "        \n",
    "        Fr_Norm = [fr_norm1, fr_norm01, fr_norm001, fr_norm0001, fr_norm00001]\n",
    "        \n",
    "        \n",
    "#         Org_Acc_df =pd.DataFrame(Org_Acc)\n",
    "#         Org_Acc_df.columns =['0.1', '0.01', '0.001', '0.0001', '0.00001']\n",
    "#         New_Acc_df =pd.DataFrame(New_Acc)\n",
    "#         New_Acc_df.columns =['0.1', '0.01', '0.001', '0.0001', '0.00001']\n",
    "#         Fr_Norm_df =pd.DataFrame(Fr_norm)\n",
    "#         Fr_Norm_df.columns =['0.1', '0.01', '0.001', '0.0001', '0.00001']\n",
    "        \n",
    "        Org_Acc_df =pd.DataFrame(Org_Acc, index =['0.1', '0.01', '0.001', '0.0001', '0.00001'], columns = ['Org_Acc'], )\n",
    "        New_Acc_df =pd.DataFrame(New_Acc, index =['0.1', '0.01', '0.001', '0.0001', '0.00001'], columns = ['New_Acc'], )\n",
    "        Fr_Norm_df =pd.DataFrame(Fr_Norm, index =['0.1', '0.01', '0.001', '0.0001', '0.00001'], columns = ['Fr_Norm'], )\n",
    "\n",
    "        ax = Org_Acc_df.plot(title=lg1+'->'+lg2+' Accuracies')\n",
    "        New_Acc_df.plot(ax=ax)\n",
    "        \n",
    "        ax = Fr_Norm_df.plot(title=lg1+'->'+lg2+' Frobenius Norm')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Models with Different Lambda Values.ipynb\r\n",
      "gensim_download.py\r\n",
      "nregularizer.py\r\n",
      "__pycache__\r\n",
      "PySpark_Comparison of Models with Different Lambda Values.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls /root/notebooks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebooks\r\n"
     ]
    }
   ],
   "source": [
    "! ls /root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
