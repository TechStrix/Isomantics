{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD w/Normalizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import nregularizer\n",
    "import numpy as np\n",
    "from gensim_download import pickle_rw\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy.linalg\n",
    "\n",
    "from numpy.linalg import det\n",
    "\n",
    "from numpy.linalg import inv\n",
    "\n",
    "import sympy as sympy\n",
    "\n",
    "from sympy import Matrix\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(vocab, vectors):\n",
    "    \"\"\"Make dictionary of vocab and vectors\"\"\"\n",
    "    return {vocab[i]: vectors[i] for i in range(len(vocab))}\n",
    "\n",
    "\n",
    "def vocab_train_test(embedding, lg1, lg2, lg1_vocab):\n",
    "    \"\"\"Create training and test vocabularies\"\"\"\n",
    "    if embedding == 'zeroshot':\n",
    "        with open('../data/zeroshot/transmat/data/' +\n",
    "                  'OPUS_en_it_europarl_train_5K.txt') as f:\n",
    "            vocab_train = [(_.split(' ')[0], _.split(' ')[1])\n",
    "                           for _ in f.read().split('\\n')[:-1]]\n",
    "        with open('../data/zeroshot/transmat/data/' +\n",
    "                  'OPUS_en_it_europarl_test.txt') as f:\n",
    "            vocab_test = [(_.split(' ')[0], _.split(' ')[1])\n",
    "                          for _ in f.read().split('\\n')[:-1]]\n",
    "\n",
    "    elif embedding in ['fasttext_random', 'fasttext_top']:\n",
    "        embedding, split = embedding.split('_')\n",
    "        lg1_lg2, lg2_lg1 = pickle_rw((lg1 + '_' + lg2, 0),\n",
    "                                     (lg2 + '_' + lg1, 0), write=False)\n",
    "        # T = Translation, R = Reverse (translated and then translated back)\n",
    "        # Create vocab from 2D translations\n",
    "        vocab_2D = []\n",
    "        for lg1_word in lg1_vocab:\n",
    "            # Translate lg1_word\n",
    "            if lg1_word in lg1_lg2:\n",
    "                lg1_word_T = lg1_lg2[lg1_word]\n",
    "\n",
    "                # Check if translated word (or lowercase) is in lg2_lg1\n",
    "                if lg1_word_T in lg2_lg1.keys():\n",
    "                    lg1_word_R = lg2_lg1[lg1_word_T]\n",
    "                elif lg1_word_T.lower() in lg2_lg1.keys():\n",
    "                    lg1_word_T = lg1_word_T.lower()\n",
    "                    lg1_word_R = lg2_lg1[lg1_word_T]\n",
    "                else:\n",
    "                    lg1_word_R = None\n",
    "\n",
    "                # Check if lg1_word and lg1_word_R are equal (lowercase)\n",
    "                if lg1_word_R:\n",
    "                    if lg1_word.lower() == lg1_word_R.lower():\n",
    "                        vocab_2D.append((lg1_word, lg1_word_T))\n",
    "        print('length of '+ lg1+'-'+ lg2+ ' vocab: '+str(len(vocab_2D)))\n",
    "\n",
    "        #Create Train/Test vocab\n",
    "\n",
    "        if split == 'random':\n",
    "            sample = np.random.choice(len(vocab_2D), 6500, replace=False)\n",
    "            vocab_train = np.asarray(vocab_2D)[sample[:5000]].tolist()\n",
    "            vocab_test = np.asarray(vocab_2D)[sample[5000:]].tolist()\n",
    "        elif split == 'top':\n",
    "            sample = np.random.choice(range(6500), 6500, replace=False)\n",
    "            vocab_train = np.asarray(vocab_2D)[:5000, :].tolist()\n",
    "            vocab_test = np.asarray(vocab_2D)[:1500, :].tolist()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # if split == 'random':\n",
    "        #     sample = np.random.choice(len(vocab_2D), 900, replace=False)\n",
    "        #     vocab_train = np.asarray(vocab_2D)[sample[:700]].tolist()\n",
    "        #     vocab_test = np.asarray(vocab_2D)[sample[700:]].tolist()\n",
    "        # elif split == 'top':\n",
    "        #     sample = np.random.choice(range(900), 900, replace=False)\n",
    "        #     vocab_train = np.asarray(vocab_2D)[:700, :].tolist()\n",
    "        #     vocab_test = np.asarray(vocab_2D)[:200, :].tolist()\n",
    "        # else:\n",
    "        #     pass\n",
    "\n",
    "    return vocab_train, vocab_test\n",
    "\n",
    "\n",
    "def vectors_train_test(vocab_train, vocab_test):\n",
    "    \"\"\"Create training and test vectors\"\"\"\n",
    "    X_train, y_train = zip(*[(lg1_dict[lg1_word], lg2_dict[lg2_word])\n",
    "                             for lg1_word, lg2_word in vocab_train])\n",
    "    X_test, y_test = zip(*[(lg1_dict[lg1_word], lg2_dict[lg2_word])\n",
    "                           for lg1_word, lg2_word in vocab_test])\n",
    "    return map(np.asarray, (X_train, X_test, y_train, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def translation_matrix(X_train, y_train):\n",
    "    \"\"\"Fit translation matrix T\"\"\"\n",
    "    #def norm_reg(weight_matrix):\n",
    "     #   return 0.01 * np.linalg.norm(np.matrix(np.subtract(np.matmul(weight_matrix,weight_matrix.T,np.matmul(weight_matrix.T,weight_matrix))),'fro')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, use_bias=False, input_shape=(X_train.shape[1],),kernel_regularizer=nregularizer.l3(0.01)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    history = model.fit(X_train, y_train, batch_size=128, epochs=20,\n",
    "                        verbose=False)\n",
    "    T = model.get_weights()[0]\n",
    "\n",
    "    T = np.matrix(T)\n",
    "\n",
    "    M = np.multiply(np.matrix(T),100)\n",
    "\n",
    "    T_norm, T_normed = normalize(M)\n",
    "\n",
    "    D = np.linalg.det(M)\n",
    "    \n",
    "    I = inv(T)\n",
    "    \n",
    "    Fr_norm = np.linalg.norm(np.matrix(np.subtract(np.matmul(T,T.getH()),np.matmul(T.getH(),T))),'fro')\n",
    "\n",
    "    print (\"Determinant:\"+str(D))\n",
    "    \n",
    "    print (\"Fr_norm:\"+str(Fr_norm))\n",
    "\n",
    "    if np.array_equal(np.around(np.matmul(T_normed,T_normed.getH())), np.around(np.matmul(T_normed.getH(),T_normed))) == True:\n",
    "        tf = \"True\"\n",
    "    else:\n",
    "        tf = \"False\"\n",
    "\n",
    "    return model, history, T, D, tf, I, M\n",
    "\n",
    "def translation_accuracy(X_test, y_test):\n",
    "    \"\"\"Get predicted matrix 'yhat' using 'T' and find translation accuracy\"\"\"\n",
    "    # yhat\n",
    "    yhat = X.dot(T)\n",
    "    count = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if yhat[i,:].all() == y_test[i,:].all():\n",
    "            count = count + 1\n",
    "    accuracy = count/len(y_test)*100\n",
    "    return accuracy\n",
    "\n",
    "def svd(T):\n",
    "    \"\"\"Perform SVD on the translation matrix 'T' \"\"\"\n",
    "    U, s, Vh = numpy.linalg.svd(T, full_matrices=False )\n",
    "    return U, s, Vh\n",
    "\n",
    "def T_svd_EDA(s):\n",
    "    \"\"\"Perform SVD on the translation matrix 'T' \"\"\"\n",
    "    plt.hist(s, bins='auto', range = (0,1),normed = 1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize(matrix):\n",
    "    \"\"\"Normalize the rows of a matrix\"\"\"\n",
    "    matrix_norm = np.linalg.norm(matrix, axis=1)\n",
    "    matrix_normed = matrix / np.repeat(matrix_norm, matrix.shape[1]). \\\n",
    "        reshape(matrix.shape)\n",
    "    return matrix_norm, matrix_normed\n",
    "\n",
    "\n",
    "def translation_results(X, y, vocab, M, lg2_vectors, lg2_vocab):\n",
    "    \"\"\"X, y, vocab - The training or test data that you want results for\n",
    "    T - The translation matrix\n",
    "    lg2_vectors, lg2_vocab - Foreign language used to find the nearest neighbor\n",
    "    \"\"\"\n",
    "\n",
    "    # Data Prep on Inputs\n",
    "    X_word, y_word = zip(*vocab)\n",
    "    X_norm, X_normed = normalize(X)\n",
    "    y_norm, y_normed = normalize(y)\n",
    "    lg2_vectors_norm, lg2_vectors_normed = normalize(lg2_vectors)\n",
    "\n",
    "    # yhat\n",
    "    yhat = X.dot(M)\n",
    "    yhat_norm, yhat_normed = normalize(yhat)\n",
    "\n",
    "    #X_norm = normalize(X)\n",
    "\n",
    "\n",
    "    # Nearest Neighbors\n",
    "#     neg_cosine = -yhat_normed.dot(lg2_vectors_normed.T)\n",
    "#     ranked_neighbor_indices = np.argsort(neg_cosine, axis=1)\n",
    "\n",
    "#     # Nearest Neighbor\n",
    "#     nearest_neighbor_indices = ranked_neighbor_indices[:, 0]\n",
    "#     yhat_neighbor = lg2_vectors[nearest_neighbor_indices, :]\n",
    "#     yhat_neighbor_norm, yhat_neighbor_normed = normalize(yhat_neighbor)\n",
    "#     yhat_neighbor_word = np.asarray(lg2_vocab)[nearest_neighbor_indices]\n",
    "\n",
    "#     # Results DF\n",
    "#     cols = ['X_norm', 'y_norm', 'yhat_norm', 'yhat_neighbor_norm',\n",
    "#             'X_word', 'y_word', 'yhat_neighbor_word']\n",
    "#     results_df = pd.DataFrame({'X_norm': X_norm,\n",
    "#                                'y_norm': y_norm,\n",
    "#                                'yhat_norm': yhat_norm,\n",
    "#                                'yhat_neighbor_norm': yhat_neighbor_norm,\n",
    "#                                'X_word': X_word,\n",
    "#                                'y_word': y_word,\n",
    "#                                'yhat_neighbor_word': yhat_neighbor_word,})\n",
    "#     results_df = results_df[cols]\n",
    "#     results_df['neighbor_correct'] = results_df.y_word == \\\n",
    "#         results_df.yhat_neighbor_word\n",
    "\n",
    "    return yhat_norm\n",
    "\n",
    "\n",
    "def T_norm_EDA(results_df):\n",
    "    \"\"\"Plot result norms side-by-side\"\"\"\n",
    "    test_size = results_df.shape[0]\n",
    "    test_accuracy = round(results_df.neighbor_correct.mean(), 2)\n",
    "\n",
    "    print('Test Accuracy: '+str(test_accuracy)+'\\n')\n",
    "\n",
    "    plot_data = ['X_norm', 'y_norm', 'yhat_norm', 'yhat_neighbor_norm']\n",
    "    # f, ax = plt.subplots(len(plot_data), sharex=True, sharey=True,\n",
    "    #                      figsize=(10, 10))\n",
    "    # for i, d in enumerate(plot_data):\n",
    "    #     ax[i].hist(results_df[d], bins=100)\n",
    "    #     ax[i].axis('off')\n",
    "    #     title = '{}: mean={}, std={}'.format(d, round(results_df[d].mean(), 2), round(results_df[d].std(), 2))\n",
    "    #     ax[i].set_title(title)\n",
    "    # f.subplots_adjust(hspace=0.7)\n",
    "    # plt.savefig('../images/' + lg1 + '_' + lg2 + '_' + embedding +\n",
    "    #             '_T_norm.png')\n",
    "    # plt.close('all')\n",
    "    return\n",
    "\n",
    "\n",
    "def T_pca_EDA(T):\n",
    "    \"\"\"PCA on matrix T\"\"\"\n",
    "    T_ss = StandardScaler().fit_transform(T)\n",
    "    pca = PCA().fit(T_ss)\n",
    "    n = pca.n_components_\n",
    "\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.xlim((0, n))\n",
    "    # plt.ylim((0, 1))\n",
    "    # plt.plot(range(n + 1), [0] + np.cumsum(pca.explained_variance_ratio_).\n",
    "    #          tolist())\n",
    "    # plt.plot(range(n + 1), np.asarray(range(n + 1)) / n)\n",
    "    # plt.xlabel('Number of Eigenvectors')\n",
    "    # plt.ylabel('Explained Variance')\n",
    "    # plt.savefig('../images/' + lg1 + '_' + lg2 + '_' + embedding +\n",
    "    #             '_T_isotropy.png')\n",
    "    # plt.close('all')\n",
    "\n",
    "    isotropy = (1 - sum(np.cumsum(pca.explained_variance_ratio_) * 1 / n)) / .5\n",
    "    return isotropy\n",
    "\n",
    "\n",
    "def T_report_results(embedding, lg1, lg2, lg1_vectors, lg2_vectors,\n",
    "                     X_train, X_test, D, results_df, isotropy):\n",
    "    md = '## ' + lg1.title() + ' to ' + lg2.title() + ' ' + \\\n",
    "        embedding.title() + '  \\n'\n",
    "    md += '- ' + lg1.title() + ' Vocabulary Size = ' + \\\n",
    "        '{:,.0f}'.format(lg1_vectors.shape[0]) + '  \\n'\n",
    "    md += '- ' + lg1.title() + ' Embedding Length = ' + \\\n",
    "        '{:,.0f}'.format(lg1_vectors.shape[1]) + '  \\n'\n",
    "    md += '- ' + lg2.title() + ' Vocabulary Size = ' + \\\n",
    "        '{:,.0f}'.format(lg2_vectors.shape[0]) + '  \\n'\n",
    "    md += '- ' + lg2.title() + ' Embedding Length = ' + \\\n",
    "        '{:,.0f}'.format(lg2_vectors.shape[1]) + '  \\n'\n",
    "    md += '- Train Size = ' + '{:,.0f}'.format(X_train.shape[0]) + '  \\n'\n",
    "    md += '- Test Size = ' + '{:,.0f}'.format(X_test.shape[0]) + '  \\n'\n",
    "    md += '- Determinant = ' + '{:,.0f}'.format(D) + '  \\n'\n",
    "\n",
    "    md += '- <b>Test Accuracy = ' + \\\n",
    "        '{:,.1%}'.format(results_df.neighbor_correct.mean()) + '</b>  \\n\\n'\n",
    "\n",
    "\n",
    "\n",
    "    md += '#### Test L2 Norms  \\n'\n",
    "    md += '- X_norm: L2 norms for ' + lg1.title() + ' test vectors  \\n'\n",
    "    md += '- y_norm: L2 norms for ' + lg2.title() + ' test vectors  \\n'\n",
    "    md += '- yhat_norm: L2 norms for X.dot(T) test vectors ' + \\\n",
    "        '(T = translation matrix)  \\n'\n",
    "    md += '- yhat_neighbor norm: L2 norms for nearest neighbor' + \\\n",
    "        'to X.dot(T) in y test vectors  \\n'\n",
    "    md += '![](../images/' + lg1 + '_' + lg2 + '_' + embedding + \\\n",
    "        '_T_norm.png)  \\n\\n'\n",
    "\n",
    "    md += '#### Translation Matrix Isotropy  \\n'\n",
    "    md += '- Isotropy = ' + '{:,.1%}'.format(isotropy) + '  \\n'\n",
    "    md += '![](../images/' + lg1 + '_' + lg2 + '_' + embedding + \\\n",
    "        '_T_isotropy.png)  \\n\\n'\n",
    "\n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function ( with SVD stats )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: en->en\n",
      "\n",
      "length of en-en vocab: 49999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/numpy/linalg/linalg.py:1821: RuntimeWarning:\n",
      "\n",
      "overflow encountered in det\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant:inf\n",
      "Fr_norm:0.184914\n",
      "biggest element:0.55252\n",
      "\n",
      "min: 0.0224942\n",
      "\n",
      "max: 1.02337\n",
      "\n",
      "mean: 0.546574\n",
      "\n",
      "median: 0.538879\n",
      "\n",
      "std: 0.23743\n",
      "\n",
      "min: -1.64793\n",
      "\n",
      "max: 0.0100318\n",
      "\n",
      "mean: -0.317995\n",
      "\n",
      "median: -0.268509\n",
      "\n",
      "std: 0.246588\n",
      "\n",
      "Translation: en->ru\n",
      "\n",
      "length of en-ru vocab: 6839\n",
      "Determinant:inf\n",
      "Fr_norm:0.276239\n",
      "biggest element:0.141753\n",
      "\n",
      "min: 0.00834662\n",
      "\n",
      "max: 1.33827\n",
      "\n",
      "mean: 0.509016\n",
      "\n",
      "median: 0.522456\n",
      "\n",
      "std: 0.203292\n",
      "\n",
      "min: -2.07849\n",
      "\n",
      "max: 0.126545\n",
      "\n",
      "mean: -0.341604\n",
      "\n",
      "median: -0.281952\n",
      "\n",
      "std: 0.237289\n",
      "\n",
      "Translation: en->de\n",
      "\n",
      "length of en-de vocab: 16693\n",
      "Determinant:inf\n",
      "Fr_norm:0.275924\n",
      "biggest element:0.13229\n",
      "\n",
      "min: 0.0120662\n",
      "\n",
      "max: 1.17449\n",
      "\n",
      "mean: 0.51275\n",
      "\n",
      "median: 0.525183\n",
      "\n",
      "std: 0.199662\n",
      "\n",
      "min: -1.91843\n",
      "\n",
      "max: 0.0698501\n",
      "\n",
      "mean: -0.336697\n",
      "\n",
      "median: -0.279692\n",
      "\n",
      "std: 0.234066\n",
      "\n",
      "Translation: en->es\n",
      "\n",
      "length of en-es vocab: 10422\n",
      "Determinant:-inf\n",
      "Fr_norm:0.258847\n",
      "biggest element:0.131187\n",
      "\n",
      "min: 0.00318155\n",
      "\n",
      "max: 1.03138\n",
      "\n",
      "mean: 0.505933\n",
      "\n",
      "median: 0.525692\n",
      "\n",
      "std: 0.194671\n",
      "\n",
      "min: -2.49736\n",
      "\n",
      "max: 0.0134181\n",
      "\n",
      "mean: -0.347938\n",
      "\n",
      "median: -0.279271\n",
      "\n",
      "std: 0.262916\n",
      "\n",
      "Translation: en->zh-CN\n",
      "\n",
      "length of en-zh-CN vocab: 1460\n",
      "Determinant:-inf\n",
      "Fr_norm:0.579581\n",
      "biggest element:0.127376\n",
      "\n",
      "min: 0.0107561\n",
      "\n",
      "max: 0.975079\n",
      "\n",
      "mean: 0.608096\n",
      "\n",
      "median: 0.635758\n",
      "\n",
      "std: 0.227377\n",
      "\n",
      "min: -1.96835\n",
      "\n",
      "max: -0.0109601\n",
      "\n",
      "mean: -0.264648\n",
      "\n",
      "median: -0.196708\n",
      "\n",
      "std: 0.245932\n",
      "\n",
      "Translation: ru->en\n",
      "\n",
      "length of ru-en vocab: 6839\n",
      "Determinant:inf\n",
      "Fr_norm:0.280117\n",
      "biggest element:0.106268\n",
      "\n",
      "min: 0.0101033\n",
      "\n",
      "max: 0.84894\n",
      "\n",
      "mean: 0.449897\n",
      "\n",
      "median: 0.468303\n",
      "\n",
      "std: 0.175605\n",
      "\n",
      "min: -1.99554\n",
      "\n",
      "max: -0.0711231\n",
      "\n",
      "mean: -0.397328\n",
      "\n",
      "median: -0.329474\n",
      "\n",
      "std: 0.245747\n",
      "\n",
      "Translation: ru->ru\n",
      "\n",
      "length of ru-ru vocab: 50000\n",
      "Determinant:inf\n",
      "Fr_norm:0.198806\n",
      "biggest element:0.590715\n",
      "\n",
      "min: 0.00842913\n",
      "\n",
      "max: 1.04263\n",
      "\n",
      "mean: 0.530339\n",
      "\n",
      "median: 0.526082\n",
      "\n",
      "std: 0.24203\n",
      "\n",
      "min: -2.07422\n",
      "\n",
      "max: 0.0181285\n",
      "\n",
      "mean: -0.338902\n",
      "\n",
      "median: -0.278961\n",
      "\n",
      "std: 0.27019\n",
      "\n",
      "Translation: ru->de\n",
      "\n",
      "length of ru-de vocab: 5910\n",
      "Determinant:inf\n",
      "Fr_norm:0.291895\n",
      "biggest element:0.114072\n",
      "\n",
      "min: 0.0206185\n",
      "\n",
      "max: 0.9422\n",
      "\n",
      "mean: 0.465258\n",
      "\n",
      "median: 0.486462\n",
      "\n",
      "std: 0.18281\n",
      "\n",
      "min: -1.68574\n",
      "\n",
      "max: -0.0258571\n",
      "\n",
      "mean: -0.381969\n",
      "\n",
      "median: -0.312951\n",
      "\n",
      "std: 0.239988\n",
      "\n",
      "Translation: ru->es\n",
      "\n",
      "length of ru-es vocab: 5870\n",
      "Determinant:inf\n",
      "Fr_norm:0.283024\n",
      "biggest element:0.138407\n",
      "\n",
      "min: 0.00211042\n",
      "\n",
      "max: 0.83818\n",
      "\n",
      "mean: 0.454005\n",
      "\n",
      "median: 0.471267\n",
      "\n",
      "std: 0.17714\n",
      "\n",
      "min: -2.67563\n",
      "\n",
      "max: -0.0766625\n",
      "\n",
      "mean: -0.396709\n",
      "\n",
      "median: -0.326733\n",
      "\n",
      "std: 0.26876\n",
      "\n",
      "Translation: ru->zh-CN\n",
      "\n",
      "length of ru-zh-CN vocab: 1266\n",
      "Determinant:inf\n",
      "Fr_norm:0.727252\n",
      "biggest element:0.139976\n",
      "\n",
      "min: 0.00423947\n",
      "\n",
      "max: 1.01667\n",
      "\n",
      "mean: 0.62261\n",
      "\n",
      "median: 0.655357\n",
      "\n",
      "std: 0.2338\n",
      "\n",
      "min: -2.37269\n",
      "\n",
      "max: 0.00717832\n",
      "\n",
      "mean: -0.258569\n",
      "\n",
      "median: -0.183523\n",
      "\n",
      "std: 0.2695\n",
      "\n",
      "Translation: de->en\n",
      "\n",
      "length of de-en vocab: 16693\n",
      "Determinant:inf\n",
      "Fr_norm:0.274582\n",
      "biggest element:0.134368\n",
      "\n",
      "min: 0.00137787\n",
      "\n",
      "max: 1.05131\n",
      "\n",
      "mean: 0.471495\n",
      "\n",
      "median: 0.486817\n",
      "\n",
      "std: 0.1824\n",
      "\n",
      "min: -2.86079\n",
      "\n",
      "max: 0.0217305\n",
      "\n",
      "mean: -0.378984\n",
      "\n",
      "median: -0.312636\n",
      "\n",
      "std: 0.268428\n",
      "\n",
      "Translation: de->ru\n",
      "\n",
      "length of de-ru vocab: 5910\n",
      "Determinant:inf\n",
      "Fr_norm:0.284448\n",
      "biggest element:0.139731\n",
      "\n",
      "min: 0.0135627\n",
      "\n",
      "max: 1.14856\n",
      "\n",
      "mean: 0.490703\n",
      "\n",
      "median: 0.505695\n",
      "\n",
      "std: 0.194644\n",
      "\n",
      "min: -1.86765\n",
      "\n",
      "max: 0.0601542\n",
      "\n",
      "mean: -0.358995\n",
      "\n",
      "median: -0.296112\n",
      "\n",
      "std: 0.242245\n",
      "\n",
      "Translation: de->de\n",
      "\n",
      "length of de-de vocab: 50000\n",
      "Determinant:inf\n",
      "Fr_norm:0.196578\n",
      "biggest element:0.577176\n",
      "\n",
      "min: 0.00549845\n",
      "\n",
      "max: 1.02551\n",
      "\n",
      "mean: 0.541814\n",
      "\n",
      "median: 0.527069\n",
      "\n",
      "std: 0.235107\n",
      "\n",
      "min: -2.25976\n",
      "\n",
      "max: 0.0109392\n",
      "\n",
      "mean: -0.324995\n",
      "\n",
      "median: -0.278133\n",
      "\n",
      "std: 0.266446\n",
      "\n",
      "Translation: de->es\n",
      "\n",
      "length of de-es vocab: 11456\n",
      "Determinant:-inf\n",
      "Fr_norm:0.280528\n",
      "biggest element:0.120892\n",
      "\n",
      "min: 0.0177174\n",
      "\n",
      "max: 0.878106\n",
      "\n",
      "mean: 0.471226\n",
      "\n",
      "median: 0.491875\n",
      "\n",
      "std: 0.182592\n",
      "\n",
      "min: -1.7516\n",
      "\n",
      "max: -0.0564531\n",
      "\n",
      "mean: -0.374156\n",
      "\n",
      "median: -0.308146\n",
      "\n",
      "std: 0.232385\n",
      "\n",
      "Translation: de->zh-CN\n",
      "\n",
      "length of de-zh-CN vocab: 1820\n",
      "Determinant:inf\n",
      "Fr_norm:0.477552\n",
      "biggest element:0.130342\n",
      "\n",
      "min: 0.013506\n",
      "\n",
      "max: 1.04984\n",
      "\n",
      "mean: 0.590899\n",
      "\n",
      "median: 0.618633\n",
      "\n",
      "std: 0.225175\n",
      "\n",
      "min: -1.86947\n",
      "\n",
      "max: 0.0211223\n",
      "\n",
      "mean: -0.278571\n",
      "\n",
      "median: -0.208567\n",
      "\n",
      "std: 0.247974\n",
      "\n",
      "Translation: es->en\n",
      "\n",
      "length of es-en vocab: 10422\n",
      "Determinant:-inf\n",
      "Fr_norm:0.266054\n",
      "biggest element:0.112154\n",
      "\n",
      "min: 0.0149778\n",
      "\n",
      "max: 0.906301\n",
      "\n",
      "mean: 0.491979\n",
      "\n",
      "median: 0.50981\n",
      "\n",
      "std: 0.188363\n",
      "\n",
      "min: -1.82455\n",
      "\n",
      "max: -0.0427274\n",
      "\n",
      "mean: -0.355237\n",
      "\n",
      "median: -0.292592\n",
      "\n",
      "std: 0.234661\n",
      "\n",
      "Translation: es->ru\n",
      "\n",
      "length of es-ru vocab: 5870\n",
      "Determinant:-inf\n",
      "Fr_norm:0.28144\n",
      "biggest element:0.129606\n",
      "\n",
      "min: 0.00555722\n",
      "\n",
      "max: 1.20648\n",
      "\n",
      "mean: 0.504892\n",
      "\n",
      "median: 0.515364\n",
      "\n",
      "std: 0.203361\n",
      "\n",
      "min: -2.25514\n",
      "\n",
      "max: 0.0815216\n",
      "\n",
      "mean: -0.347084\n",
      "\n",
      "median: -0.287888\n",
      "\n",
      "std: 0.245461\n",
      "\n",
      "Translation: es->de\n",
      "\n",
      "length of es-de vocab: 11456\n",
      "Determinant:inf\n",
      "Fr_norm:0.276824\n",
      "biggest element:0.132298\n",
      "\n",
      "min: 0.00204203\n",
      "\n",
      "max: 1.08006\n",
      "\n",
      "mean: 0.501876\n",
      "\n",
      "median: 0.512106\n",
      "\n",
      "std: 0.202549\n",
      "\n",
      "min: -2.68994\n",
      "\n",
      "max: 0.0334477\n",
      "\n",
      "mean: -0.35436\n",
      "\n",
      "median: -0.29064\n",
      "\n",
      "std: 0.270535\n",
      "\n",
      "Translation: es->es\n",
      "\n",
      "length of es-es vocab: 50000\n",
      "Determinant:-inf\n",
      "Fr_norm:0.182688\n",
      "biggest element:0.561974\n",
      "\n",
      "min: 0.00541086\n",
      "\n",
      "max: 1.04104\n",
      "\n",
      "mean: 0.544559\n",
      "\n",
      "median: 0.528836\n",
      "\n",
      "std: 0.240431\n",
      "\n",
      "min: -2.26673\n",
      "\n",
      "max: 0.0174666\n",
      "\n",
      "mean: -0.323687\n",
      "\n",
      "median: -0.276679\n",
      "\n",
      "std: 0.265321\n",
      "\n",
      "Translation: es->zh-CN\n",
      "\n",
      "length of es-zh-CN vocab: 1164\n",
      "Determinant:-inf\n",
      "Fr_norm:0.723496\n",
      "biggest element:0.129009\n",
      "\n",
      "min: 0.0118905\n",
      "\n",
      "max: 1.00203\n",
      "\n",
      "mean: 0.627637\n",
      "\n",
      "median: 0.65965\n",
      "\n",
      "std: 0.233992\n",
      "\n",
      "min: -1.9248\n",
      "\n",
      "max: 0.000882487\n",
      "\n",
      "mean: -0.251695\n",
      "\n",
      "median: -0.180686\n",
      "\n",
      "std: 0.25045\n",
      "\n",
      "Translation: zh-CN->en\n",
      "\n",
      "length of zh-CN-en vocab: 1460\n",
      "Determinant:-inf\n",
      "Fr_norm:0.600317\n",
      "biggest element:0.138766\n",
      "\n",
      "min: 0.0344044\n",
      "\n",
      "max: 1.04585\n",
      "\n",
      "mean: 0.649524\n",
      "\n",
      "median: 0.684112\n",
      "\n",
      "std: 0.238099\n",
      "\n",
      "min: -1.46339\n",
      "\n",
      "max: 0.0194689\n",
      "\n",
      "mean: -0.231991\n",
      "\n",
      "median: -0.164873\n",
      "\n",
      "std: 0.227587\n",
      "\n",
      "Translation: zh-CN->ru\n",
      "\n",
      "length of zh-CN-ru vocab: 1266\n",
      "Determinant:inf\n",
      "Fr_norm:0.718869\n",
      "biggest element:0.1377\n",
      "\n",
      "min: 0.0107387\n",
      "\n",
      "max: 1.02909\n",
      "\n",
      "mean: 0.648619\n",
      "\n",
      "median: 0.684081\n",
      "\n",
      "std: 0.242259\n",
      "\n",
      "min: -1.96905\n",
      "\n",
      "max: 0.0124533\n",
      "\n",
      "mean: -0.238549\n",
      "\n",
      "median: -0.164892\n",
      "\n",
      "std: 0.255057\n",
      "\n",
      "Translation: zh-CN->de\n",
      "\n",
      "length of zh-CN-de vocab: 1820\n",
      "Determinant:inf\n",
      "Fr_norm:0.436258\n",
      "biggest element:0.146895\n",
      "\n",
      "min: 0.0113357\n",
      "\n",
      "max: 0.999923\n",
      "\n",
      "mean: 0.630551\n",
      "\n",
      "median: 0.665634\n",
      "\n",
      "std: 0.233382\n",
      "\n",
      "min: -1.94555\n",
      "\n",
      "max: -3.36272e-05\n",
      "\n",
      "mean: -0.246171\n",
      "\n",
      "median: -0.176764\n",
      "\n",
      "std: 0.235154\n",
      "\n",
      "Translation: zh-CN->es\n",
      "\n",
      "length of zh-CN-es vocab: 1164\n",
      "Determinant:-inf\n",
      "Fr_norm:0.719634\n",
      "biggest element:0.158812\n",
      "\n",
      "min: 0.0134559\n",
      "\n",
      "max: 1.04932\n",
      "\n",
      "mean: 0.650238\n",
      "\n",
      "median: 0.689871\n",
      "\n",
      "std: 0.242564\n",
      "\n",
      "min: -1.87109\n",
      "\n",
      "max: 0.0209094\n",
      "\n",
      "mean: -0.238108\n",
      "\n",
      "median: -0.161232\n",
      "\n",
      "std: 0.258121\n",
      "\n",
      "Translation: zh-CN->zh-CN\n",
      "\n",
      "length of zh-CN-zh-CN vocab: 16041\n",
      "Determinant:-inf\n",
      "Fr_norm:0.189579\n",
      "biggest element:0.525294\n",
      "\n",
      "min: 0.0103776\n",
      "\n",
      "max: 1.0382\n",
      "\n",
      "mean: 0.568138\n",
      "\n",
      "median: 0.573653\n",
      "\n",
      "std: 0.23522\n",
      "\n",
      "min: -1.9839\n",
      "\n",
      "max: 0.0162828\n",
      "\n",
      "mean: -0.299072\n",
      "\n",
      "median: -0.241351\n",
      "\n",
      "std: 0.250701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Manually set list of translations (embedding, lg1, lg2)\n",
    "    translations = [#('fasttext_random', 'en', 'ru'),\n",
    "                    ('fasttext_top', 'en', 'en'),\n",
    "                    ('fasttext_top', 'en', 'ru'),\n",
    "                    #('fasttext_random', 'en', 'de'),\n",
    "                    ('fasttext_top', 'en', 'de'),\n",
    "                    #('fasttext_random', 'en', 'es'),\n",
    "                    ('fasttext_top', 'en', 'es'),\n",
    "                    #('fasttext_random', 'en', 'zh-CN'),\n",
    "                    ('fasttext_top', 'en', 'zh-CN'),\n",
    "        \n",
    "        \n",
    "        \n",
    "                     #('fasttext_random', 'ru', 'en'),\n",
    "                    ('fasttext_top', 'ru', 'en'),\n",
    "                    #('fasttext_random', 'ru', 'es'),\n",
    "                    ('fasttext_top', 'ru', 'ru'),\n",
    "                    ('fasttext_top', 'ru', 'de'),\n",
    "                    ('fasttext_top', 'ru', 'es'),\n",
    "                    #('fasttext_random', 'ru', 'zh-CN'),\n",
    "                    ('fasttext_top', 'ru', 'zh-CN'),\n",
    "                    #('fasttext_random', 'ru', 'de'),\n",
    "                    \n",
    "        \n",
    "        \n",
    "                    #('fasttext_random', 'de', 'en'),\n",
    "                    ('fasttext_top', 'de', 'en'),\n",
    "                    #('fasttext_random', 'de', 'es'),\n",
    "                    ('fasttext_top', 'de', 'ru'), \n",
    "                    #('fasttext_random', 'de', 'ru'),\n",
    "                    ('fasttext_top', 'de', 'de'),\n",
    "                    ('fasttext_top', 'de', 'es'),\n",
    "                    #('fasttext_random', 'de', 'zh-CN'),\n",
    "                    ('fasttext_top', 'de', 'zh-CN'),\n",
    "        \n",
    "        \n",
    "        \n",
    "                    #('fasttext_random', 'es', 'en'),\n",
    "                    ('fasttext_top', 'es', 'en'),\n",
    "                    #('fasttext_random', 'es', 'de'),\n",
    "                    ('fasttext_top', 'es', 'ru'),\n",
    "                    ('fasttext_top', 'es', 'de'),\n",
    "                    #('fasttext_random', 'es', 'ru'),\n",
    "                    ('fasttext_top', 'es', 'es'),\n",
    "                    #('fasttext_random', 'es', 'zh-CN'),\n",
    "                    ('fasttext_top', 'es', 'zh-CN'),\n",
    "        \n",
    "                \n",
    "        \n",
    "                    #('fasttext_random', 'zh-CN', 'en'),\n",
    "                    ('fasttext_top', 'zh-CN', 'en'),\n",
    "                    #('fasttext_random', 'zh-CN', 'es'),\n",
    "                    ('fasttext_top', 'zh-CN', 'ru'),\n",
    "                    ('fasttext_top', 'zh-CN', 'de'),\n",
    "                    ('fasttext_top', 'zh-CN', 'es'),\n",
    "                    #('fasttext_random', 'zh-CN', 'ru'),\n",
    "                    ('fasttext_top', 'zh-CN', 'zh-CN'),\n",
    "                    #('fasttext_random', 'zh-CN', 'de'),\n",
    "        \n",
    "                    \n",
    "                    \n",
    "\n",
    "                    ]\n",
    "   \n",
    "    s_min_en=[]\n",
    "    s_max_en=[]\n",
    "    s_mean_en=[]\n",
    "    s_median_en=[]\n",
    "    s_std_en=[]\n",
    "            \n",
    "    s1_min_en=[]\n",
    "    s1_max_en=[]\n",
    "    s1_mean_en=[]\n",
    "    s1_median_en=[]\n",
    "    s1_std_en=[]\n",
    "        \n",
    "       \n",
    "        \n",
    "    s_min_es=[]\n",
    "    s_max_es=[]\n",
    "    s_mean_es=[]\n",
    "    s_median_es=[]\n",
    "    s_std_es=[]\n",
    "            \n",
    "    s1_min_es=[]\n",
    "    s1_max_es=[]\n",
    "    s1_mean_es=[]\n",
    "    s1_median_es=[]\n",
    "    s1_std_es=[]\n",
    "            \n",
    "       \n",
    "    s_min_ru=[]\n",
    "    s_max_ru=[]\n",
    "    s_mean_ru=[]\n",
    "    s_median_ru=[]\n",
    "    s_std_ru=[]\n",
    "            \n",
    "    s1_min_ru=[]\n",
    "    s1_max_ru=[]\n",
    "    s1_mean_ru=[]\n",
    "    s1_median_ru=[]\n",
    "    s1_std_ru=[]\n",
    "            \n",
    "       \n",
    "        \n",
    "    s_min_de=[]\n",
    "    s_max_de=[]\n",
    "    s_mean_de=[]\n",
    "    s_median_de=[]\n",
    "    s_std_de=[]\n",
    "        \n",
    "    s1_min_de=[]\n",
    "    s1_max_de=[]\n",
    "    s1_mean_de=[]\n",
    "    s1_median_de=[]\n",
    "    s1_std_de=[]\n",
    "        \n",
    "      \n",
    "        \n",
    "    s_min_zh=[]\n",
    "    s_max_zh=[]\n",
    "    s_mean_zh=[]\n",
    "    s_median_zh=[]\n",
    "    s_std_zh=[]\n",
    "            \n",
    "    s1_min_zh=[]\n",
    "    s1_max_zh=[]\n",
    "    s1_mean_zh=[]\n",
    "    s1_median_zh=[]\n",
    "    s1_std_zh=[]\n",
    "\n",
    "    md = ''\n",
    "    for translation in translations:\n",
    "        embedding, lg1, lg2 = translation\n",
    "        # Vocab/Vectors/Dicts\n",
    "        lg1_vocab, lg1_vectors, lg2_vocab, lg2_vectors = \\\n",
    "            pickle_rw((lg1 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg1 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      write=False)\n",
    "        lg1_dict = make_dict(lg1_vocab, lg1_vectors)\n",
    "        lg2_dict = make_dict(lg2_vocab, lg2_vectors)\n",
    "\n",
    "        print('Translation: '+lg1+'->'+lg2+'\\n')\n",
    "\n",
    "        # Train/Test Vocab/Vectors\n",
    "        vocab_train, vocab_test = vocab_train_test(embedding, lg1, lg2, lg1_vocab)\n",
    "        X_train, X_test, y_train, y_test = vectors_train_test(vocab_train,\n",
    "                                                              vocab_test)\n",
    " \n",
    "        \n",
    "        # Fit tranlation matrix to training data\n",
    "        model, history, T, D, tf,I, M = translation_matrix(X_train, y_train)\n",
    "\n",
    "        print('biggest element:'+str(np.max(T))+'\\n')\n",
    "        \n",
    "        U,s,Vh = svd(T)\n",
    "        \n",
    "        #scaler = MinMaxScaler()\n",
    "        \n",
    "        #scaler.fit(s)\n",
    "        \n",
    "        #s = scaler.transform(s)\n",
    "        \n",
    "        s1 = np.log10(s)\n",
    "        \n",
    "        print(\"min: \"+str(min(s))+\"\\n\")\n",
    "        \n",
    "        print(\"max: \"+str(max(s))+\"\\n\")\n",
    "        \n",
    "        print(\"mean: \"+str(np.mean(s))+\"\\n\")\n",
    "        \n",
    "        print(\"median: \"+str(np.median(s))+\"\\n\")\n",
    "        \n",
    "        print(\"std: \"+str(np.std(s))+\"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"min: \"+str(min(s1))+\"\\n\")\n",
    "        \n",
    "        print(\"max: \"+str(max(s1))+\"\\n\")\n",
    "        \n",
    "        print(\"mean: \"+str(np.mean(s1))+\"\\n\")\n",
    "        \n",
    "        print(\"median: \"+str(np.median(s1))+\"\\n\")\n",
    "        \n",
    "        print(\"std: \"+str(np.std(s1))+\"\\n\")\n",
    "        \n",
    "        \n",
    "        if lg1 == 'en':\n",
    "            s_min_en.append(min(s))\n",
    "            s_max_en.append(max(s))\n",
    "            s_mean_en.append(np.mean(s))\n",
    "            s_median_en.append(np.median(s))\n",
    "            s_std_en.append(np.std(s))\n",
    "            \n",
    "            s1_min_en.append(min(s1))\n",
    "            s1_max_en.append(max(s1))\n",
    "            s1_mean_en.append(np.mean(s1))\n",
    "            s1_median_en.append(np.median(s1))\n",
    "            s1_std_en.append(np.std(s1))\n",
    "        \n",
    "        if lg1 == 'es':\n",
    "        \n",
    "            s_min_es.append(min(s))\n",
    "            s_max_es.append(max(s))\n",
    "            s_mean_es.append(np.mean(s))\n",
    "            s_median_es.append(np.median(s))\n",
    "            s_std_es.append(np.std(s))\n",
    "            \n",
    "            s1_min_es.append(min(s1))\n",
    "            s1_max_es.append(max(s1))\n",
    "            s1_mean_es.append(np.mean(s1))\n",
    "            s1_median_es.append(np.median(s1))\n",
    "            s1_std_es.append(np.std(s1))\n",
    "            \n",
    "        if lg1 == 'ru':\n",
    "        \n",
    "            s_min_ru.append(min(s))\n",
    "            s_max_ru.append(max(s))\n",
    "            s_mean_ru.append(np.mean(s))\n",
    "            s_median_ru.append(np.median(s))\n",
    "            s_std_ru.append(np.std(s))\n",
    "            \n",
    "            s1_min_ru.append(min(s1))\n",
    "            s1_max_ru.append(max(s1))\n",
    "            s1_mean_ru.append(np.mean(s1))\n",
    "            s1_median_ru.append(np.median(s1))\n",
    "            s1_std_ru.append(np.std(s1))\n",
    "            \n",
    "        if lg1 == 'de':\n",
    "        \n",
    "            s_min_de.append(min(s))\n",
    "            s_max_de.append(max(s))\n",
    "            s_mean_de.append(np.mean(s))\n",
    "            s_median_de.append(np.median(s))\n",
    "            s_std_de.append(np.std(s))\n",
    "            \n",
    "            s1_min_de.append(min(s1))\n",
    "            s1_max_de.append(max(s1))\n",
    "            s1_mean_de.append(np.mean(s1))\n",
    "            s1_median_de.append(np.median(s1))\n",
    "            s1_std_de.append(np.std(s1))\n",
    "       \n",
    "        if lg1 == 'zh-CN':\n",
    "        \n",
    "            s_min_zh.append(min(s))\n",
    "            s_max_zh.append(max(s))\n",
    "            s_mean_zh.append(np.mean(s))\n",
    "            s_median_zh.append(np.median(s))\n",
    "            s_std_zh.append(np.std(s))\n",
    "            \n",
    "            s1_min_zh.append(min(s1))\n",
    "            s1_max_zh.append(max(s1))\n",
    "            s1_mean_zh.append(np.mean(s1))\n",
    "            s1_median_zh.append(np.median(s1))\n",
    "            s1_std_zh.append(np.std(s1))\n",
    "    \n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         #print(s)\n",
    "        \n",
    "#         plt.hist(s,bins='auto')#bins=50,normed='True',range = (0.0,0.2))\n",
    "#         #plt.plot(s)\n",
    "#         plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
